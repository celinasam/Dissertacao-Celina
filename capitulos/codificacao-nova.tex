\chapter{Codificação por Apagamento}

A transmissão e o armazenamento de dados tem muito em comum. Ambos transferem dados da fonte para o destino. Para garantir confiabilidade nessas operações, é utilizada codificação por apagamento ou códigos corretores de erros. A Teoria dos Códigos tem sido estudada há décadas: por matemáticos nas décadas de 50 e 60 e a partir da década de 70 por engenheiros \cite{Ritter:2009, Hefez:2008}.

Com a popularização dos computadores e as pesquisas espaciais, os códigos corretores tornaram-se parte comum de comunicações por satélite, de redes de computadores, de armazenamento em discos óticos e outros meios magnéticos. A presença dos códigos corretores de erros é frequente em nosso cotidiano: quando se assiste a um programa de televisão, quando se ouve música a partir de um CD, quando se faz um telefonema, quando se assiste um filme gravado em DVD, quando se navega pela internet.

A codificação de mensagens no emissor antes da transmissão e a
decodificação das mensagens (possivelmente danificadas) que chegam ao
receptor, possibilita reparar os efeitos de um canal físico com ruídos
\cite{Shannon:1948} sem sobrecarregar a taxa de transmissão de
informação ou o \emph{overhead} de armazenamento \cite{Lin:1983}.

% A idéia básica da codificação ótima por apagamento é que o objeto
% original possa ser reconstruído a partir de quaisquer $k$ únicos
% fragmentos que são aproximadamente do mesmo tamanho do objeto original
% \cite{Weatherspoon:2002:01}.
%Para uma codificação quase ótima, são
% necessários $(1+e) \times m$ fragmentos, onde $e \geq 1$
% \cite{EC:2010}.

Um dos principais parâmetros de um código para detecção de erros é a
probabilidade de detecção de erro.

 A probabilidade de erro no canal determina a capacidade de
 transferência de informação no canal. Os modelos estudados por
 pesquisadores envolvem canais simétricos, assimétricos e outros, com
 ou sem memória~\cite{Weber:1985}. 

%Em canais binários simétricos, assume-se que ambos os erros $0 \rightarrow 1$ e $1 \rightarrow 0$ ocorrem com igual probabilidade.

\section{Shannon: conceitos e teoremas fundamentais}

Shannon introduziu dois conceitos fundamentais sobre informação que é transmitida em um sistema de comunicação \cite{Yeung:2008}:

\begin{description}
   \item [a incerteza da informação] se o dado que nos interessa é determinístico, então ele não tem valor algum. Por exemplo, a transmissão contínua de uma imagem em um sistema de televisão é supérflua. Desta forma, a fonte de informação é modelada por uma variável ou processo aleatório e uma probabilidade é utilizada para se desenvolver a teoria da informação.
   \item [a informação transmitida é digital] o dado que nos interessa deve ser convertido em \emph{bits} e ser entregue no destino corretamente, sem referência ao seu significado inicial. O trabalho do Shannon \cite{Shannon:1948} parece ser o primeiro trabalho publicado que usa o termo \emph{bit}.
\end{description}

Nesse mesmo trabalho, Shannon demonstrou dois importantes teoremas que são fundamentais na comunicação ponto-a-ponto:

\begin{description}
   \item [\emph{source coding theorem}] introduz a entropia como medida da informação que, nesse caso, é caracterizada por a taxa mínima de código que representa uma informação livre de erros. Este teorema é a base téorica para compressão de dados.
   \item [\emph{channel coding theorem}] fala da capacidade de um canal com ruídos, na qual a informação é transmitida de forma confiável, desde que ritmo de transferência de dados seja menor que a capacidade do canal. 
\end{description}

A probabilidade de erro no canal determina a capacidade de transferência de informação no canal.

\section{Canais: modelos e erros}

O código da fonte é o conjunto de elementos que definem forma como a informação será transmitida, por exemplo, em termos de \emph{bits} e o código do canal inclui o código da fonte e a redundância (por exemplo, em termos de \emph{bits}) introduzida para garantir a correção de erros.

Uma dificuldade encontrada por quem estuda códigos corretores de erros é que não existe uma nomenclatura unificada \cite{Plank:2009}. Também segundo \cite{CS540:2010}, existem poucos pesquisadores que são programadores de sistemas e que fazem propostas neste tema.

A idéia básica da código ótimo é que o objeto original possa ser reconstruído a partir de quaisquer $k$ únicos fragmentos que são aproximadamente do mesmo tamanho do objeto original \cite{Weatherspoon:2002:01}.
%Para uma codificação quase ótima, são necessários $ \approx  (1+e)k$ fragmentos, onde $e \geq 1$ \cite{EC:2010}.

Corrigir erros é uma tarefa mais complexa que detectá-los. Detectar erros tem a mesma complexidade que a operação de codificar, que pode ser linear no tamanho das palavras código. A operação de decodificar para uma correção de erros ótima é um problema NP-difícil e são conhecidos apenas algoritmos eficientes para algumas classes de códigos. Novas classes de códigos com eficientes decodificadores e novos algoritmos para decodificação para códigos conhecidos são promissoras pesquisas \cite{Klove:2007}.

%Um dos principais parâmetros de um código para detecção de erros é a probabilidade de detecção de erro. Os modelos estudados por pesquisadores envolvem canais simétricos, assimétricos e outros, com ou sem memória.

Em canais binários simétricos, assume-se que ambos os erros $0 \rightarrow 1$ e $1 \rightarrow 0$  ocorrem com igual probabilidade \cite{Weber:1985}. \index{Canal binário simétrico}

\vspace*{2cm}
\begin{figure}[h]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(0.3,4.1){\vector(1,-1){4}}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(1,3){{\scriptsize $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Canal binário simétrico \cite{Weber:1985}}
   \label{fig0:bsc}
\end{figure}
\vspace*{2cm}


Em algumas aplicações como comunicações óticas, os erros tem uma natureza assimétrica. Canais, onde esse tipo de erro ocorrem, podem ser modelados para canais binários assimétricos ou canal-Z, onde apenas erros com $1$'s ocorrem. Um exemplo disso são sistemas que utilizam \emph{photons} para transmitir informação \cite{Weber:1985}. \index{Canal binário assimétrico}

\vspace*{4cm}
\begin{figure}[h]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Canal binário assimétrico \cite{Weber:1985}}
   \label{fig00:bac}
\end{figure}
\vspace*{2cm}

Existem dois métodos básicos para tratar erros em comunicação e ambos
envolvem a codificação de mensagens. A diferença está em como esses
códigos são utilizados. Em um \emph{Automatic Repeat reQuest}, os códigos
são utilizados para detectar erros e se estes existirem, é feito um
pedido de retransmissão. Com \emph{Forward Error Correction}, os
códigos são usados para detectar e corrigir erros e não é necessário
um caminho de retorno.

\section{\emph{Automatic Repeat reQuest} - ARQ}

ARQ utiliza redundância para detectar erros em mensagens e, após a detecção, o destinatário solicita uma repetição da transmissão. Um caminho de retorno é necessário. São sistemas de \emph{two-way transmission}. Exemplos de sistemas que utilizam ARQ são linhas telefônicas e alguns sistemas de satélite \cite{Lin:1983}.

Se existe um grande atraso de propagação, uma grande distância entre emissor e destinatário, este método pode ser muito ineficiente. Podem existir casos em que a retransmissão não é possível, quando não existe \emph{backup}.

Técnicas ARQ incluem repetição seletiva, \emph{Go-back N} e sistemas de reconhecimento positivo ou negativo \cite{Kurose:2010}.

\section{\emph{Forward Correction Code} - FEC}

Técnicas FEC representam um \emph{one-way system}. A transmissão ou gravação é restrita a uma direção: da fonte para o sumidouro (destino). Sistemas de armazenamento de fita magnética e sistemas da \emph{NASA's Deep Space Network} utilizam técnicas FEC \cite{Lin:1983}.

O destinatário corrige a mensagem recebida através da codificação por apagamento. Este procedimento geralmente é chamado de correção de erros de repasse e pode ser implementado em \emph{hardware} de propósito especial.

FEC utiliza redundância, assim o decodificador pode corrigir os erros de mensagens no destinatário. Uma analogia pode ser feita com uma pessoa que fala devagar e repetidamente, em uma linha telefônica com ruídos, acrescentando mais redundância para o ouvinte entender a mensagem correta.

\subsection{Como FEC funcionam}

Técnicas FEC incluem códigos de blocos (\emph{block codes}), que são códigos sem memória e códigos convolacionais, que são códigos com memória \cite{Berlekamp:1987}. 

\subsubsection{Códigos de Blocos}

Na Figura~\ref{fig3:fec}, vemos um sistema que utiliza código de
blocos. A fonte envia uma sequência de dados para o codificador. O
codificador divide esta sequência em blocos de $k$ \emph{bits}
cada chamados mensagens.  Uma mensagem é representada por uma
$k$-tupla binária $u = u_1, u_2,\dots, u_k$. O codificador insere
\emph{bits} redundantes (ou de paridade) para cada mensagem $u$,
gerando uma sequência de saída de $m$ \emph{bits} chamada
\emph{codeword} ou palavra código representada por uma $m$-tupla de
símbolos discretos $v = v_1, v_2, \dots, v_m$.  Os $m - k$ bits são os
bits redundantes que provêm à codificação a capacidade de tratar os
ruídos do canal.

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{center}
  {\begin{picture}(12.5,6)(0,-3)
    \put(0,2){\framebox(3,1){Fonte}}
    \put(3,2.5){\vector(1,0){2}}
    \put(4,2.6){u}
    \put(5,2){\framebox(4,1){{Codificador (m,k)}}}
    \put(9,2.5){\line(1,0){2}}
    \put(10,2.6){v}
    \put(11,2.5){\vector(0,-1){2}}
    \put(9.5,-0.5){\framebox(3,1){Canal}}
    \put(6.8,-0.1){ruídos}
    \put(8,-0){\vector(1,0){1.5}}
    \put(11,-0.5){\line(0,-1){2}}
    \put(11,-2.5){\vector(-1,0){2}}
    \put(10,-2.4){r}
    \put(0,-3){\framebox(3,1){Destino}}
    \put(5,-2.5){\vector(-1,0){2}}
    \put(4,-2.4){$\mathaccent 94 u$}
    \put(5,-3){\framebox(4,1){{Decodificador (m,k)}}}
   \end{picture}}
  \end{center}
  \caption{Códigos de bloco}
  \label{fig3:fec}
\end{figure}
\vspace*{2cm}

Códigos de blocos são identificados pela notação ($m$, $k$), de acordo com o número de \emph{bits} de saída $m$ e o número de \emph{bits} $k$ de cada um dos  blocos de entrada.

Todas as palavras código de um código de blocos tem tamanho fixo, que é um certo número de blocos de  $k$ \emph{bits}. 

A geração de uma palavra código depende apenas de um cálculo algébrico entre os $k$ bits, portanto, um codificador pode ser implementado como um circuito lógico combinacional. O codificador executa o mapeamento: $T :  U \rightarrow V$ onde $U$ é um conjunto de palavras de dados de tamanho $k$ e $V$ é um conjunto de palavras código de tamanho $m$ onde $m > k$. Cada uma das $2^k$ palavras de dados é mapeada para uma única palavra código.

A taxa de codificação e a sobrecarga de armazenamento são
calculados a partir de $k$ blocos originais \cite{RTAD:2007,
  CMSC:2010}. São gerados $m$ símbolos pelo algoritmo de
codificação. $R = \frac{k}{m}$ é a taxa de codificação que pode ser
interpretada como o número de bits de informação por palavra código
transmitida e $O = \frac{1}{R}$ é a sobrecarga de armazenamento.

Se $k \leq m$, mais bits redundantes podem ser adicionados, com aumento do tamanho da palavra código, mantendo $R = \frac{k}{m}$ constante. Como escolher este número $m - k$ de bits redundantes para obter transmissão confiável em cima de um canal com ruídos é o problema principal do projeto do codificador. No destino, o decodificador extrai a sequência original de dados.

Outra métrica utilizada é a redundância que pode ser definida por
$\frac{(m - k)}{m}$. A alta redundância reduz a possibilidade de
todos os dados serem enviados em uma única transmissão. A desvantagem
da redundância é que a adição de \emph{bits} pode exigir uma largura
de banda transmissão maior ou aumentar o atraso das mensagens (ou
ambos).

Um código C é linear se v e w são palavras código distintas de um código C, então v+w é também uma palavra código de C. Um código linear contém a palavra código zero, pois $v + v = 0$. Operação simples de decodificação, pouca memória e métodos simples para determinação de padrões de erros são algumas das vantagens de códigos lineares.

Um código C é chamado de cíclico se o deslocamento cíclico (\emph{shift}) de qualquer palavra código gera uma nova palavra código. Por exemplo, $C1 = \{000, 110, 101, 011\}$ é um código cíclico. $C2 = \{000, 100, 011, 111\}$ não é um código cíclico.

Existem vários códigos de blocos \cite{Byers:1998, Kubiatowicz:2000}. Alguns dos mais utilizados são códigos Reed-Solomon (RS), códigos Low-Density Parity-Check (LDPC) e códigos Tornado \cite{Mitzenmacher:2004, RTAD:2007}.

%Segundo \cite{Almeida:2007}, códigos Reed-Solomon (RS) são
%particularmente úteis para correção de erros em rajada (seqüência
%símbolos consecutivos, nenhum desses recebidos corretamente, chamados
%\emph{burst errors}). Também podem ser usados eficientemente em canais
%em que o conjunto de símbolos de entrada é consideravelmente grande.


Segundo \cite{AF:2010}, as principais características de códigos de blocos são:

\begin{description}
   \item [taxa de codificação] $R = \frac{k}{m}$ é a medida da eficiência do código, pois é o quociente do número de \emph{bits} da palavra de dados sobre o número de \emph{bits} total da palavra transmitida
   \item [distância mínima] $(d_{min})$ é a menor distância de Hamming \footnote{A distância de Hamming (dH) entre duas palavras código $v_i$ e $v_j$  é o número de \emph{bits} que são diferentes nessas duas palavras.} entre duas quaisquer palavras do código; ela depende do número de \emph{bits} redundantes $q = m - k$, tal que  $(d_{min} \leq q + 1)$
   \item [capacidade de detecção] detecta até $l$ erros, onde $l \leq d_{min} - 1$
   \item [capacidade de correção] corrige os erros até $t$ erros, onde $ t \leq \lfloor \frac{d_{min} - 1}{2} \rfloor$
   \item [capacidade de detecção e correção] detecta até $l$ erros e corrige os erros até $t$ erros, onde $d_{min} \geq l + t + 1$ e $l > t$
\end{description}

Um código com $d_{min} = 1$ não tem capacidade de detectar erros.

Códigos Reed-Solomon são códigos de blocos, lineares e cíclicos. São códigos parametrizáveis, cuja capacidade de correção de erros pode ser alterada facilmente. \index{Codificação Reed-Solomon}

%Definition. A Reed-Solomon (or RS) code over GF(q) is a BCH (BOSE-CHAUDHURI-HOCQUENGHEM) code of length N = q - 1. Of course q is never 2. Thus the length is the number of nonzero elements in the ground field. We shall use N , K and D to denote the length, dimension, and minimum distance (using capital letters to distinguish them from the parameters of the binary codes which will be constructed later). RS codes are important in concatenated codes and burst correction.

%Em \cite{Plank:1997}, o autor apresenta uma especificação completa do problema e do algoritmo da codificação e detalhes de sua implementação. O modelo estudado é formado por $n$ dispositivos de armazenamento  $D_1, D_2, ..., D_n$ (\emph{data devices}) e outros $m$ dispositivos de armazenamento  $C_1, C_2, ..., C_m$ (\emph{checksum devices}). O conteúdo de cada um dos $m$ \emph{checksum devices} é calculado a partir do conteúdo dos $n$ \emph{data devices}. O objetivo do cálculo dos $C_i$ para $1 \leq i \leq m$ é tal que para quaisquer $m$ dispositivos que falhem dos $D_1, D_2, ..., D_n, C_1, C_2, ..., C_m$, o conteúdo dos dispositivos que falharam possa ser reconstituído a partir dos dispositivos que não falharam.

Segundo \cite{Almeida:2007}, códigos RS são particularmente úteis para correção de erros em rajada (seqüência símbolos consecutivos, nenhum desses recebidos corretamente, chamados \emph{burst errors}). Também podem ser usados eficientemente em canais onde o conjunto de símbolos de entrada é consideravelmente grande.

%Uma implementação de biblioteca em C/C++ para o algoritmo RS foi apresentada em \cite{Plank:2007}.

O sistema de armazenamento OceanStore \cite{Kubiatowicz:2000} e o protocolo BitTorrent (aplicação da camada de rede da internet) usam uma codificação RS.

\emph{Redundant Arrays of Inexpensive [Independent] Disks} (RAID) é uma classe de códigos RS. RAID é um método para prover tolerância a falhas ou alto desempenho em sistemas de armazenagem utilizando para isso uma codificação de correção de erros ou paridade. RAID foi introduzido por D. A. Patterson na Universidade da California, Berkeley (UC Berkeley) em 1988 \cite{Patterson:1988}. \index{RAID}

Segundo \cite{Woitaszek:2007}, para sistemas de armazenamento, a
codificação por apagamento baseada em operações simples, tais como XOR
RAID, são preferíveis. Embora um mecanismo externo deva ser utilizado
para detectar erros, as operações de XOR podem ser realizadas
rapidamente e resultar em alto \emph{throughput} das operações de
codificação e decodificação.

São conceitos básicos \cite{Vadala:2002}:

\begin{description}

   \item [data striping] é uma técnica para segmentar dados sequênciais, como um arquivo, de maneira que o acesso a segmentos sequênciais seja feito por diferentes dispositivos de armazenamento. Esta técnica é útil quando se quer processar mais rapidamente os pedidos de acesso a dados que os dispositivos de armazenamento permitem. Diferentes segmentos de dados são mantidos em diferentes dispositivos de armazenamento. A falha de um dos dispositivos torna toda a sequência de dados indisponível. Essa desvantagem é superada pelo armazenamento de informações redundantes (custo de armazenamento extra), como a paridade, com o objetivo de correção de erros. As configurações de RAID que utilizam paridade são RAID-2, RAID-3, RAID-4, RAID-5 e RAID-6 \cite{DS:2010}.

   \item [stripe] são segmentos consecutivos ou faixas que são escritos sequencialmente através de cada um dos discos de um \emph{array} ou conjunto. Cada segmento tem um tamanho definido em blocos.

\end{description}

\vspace{1cm}

{\bf RAID 2}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{bit} e usa códigos Hamming para correção de erros. Por exemplo, o código Hamming(7,4) (quatro \emph{bits} de dados e tres \emph{bits} de paridade) permite usar 7 discos em RAID 2, sendo 4 usados para armazenar dados e 3 usados para correção de erros. Esta codificação tornou-se padrão para \emph{hard drives} e tornou-se desnecessária, assim deixou de ser vantajosa.

\vspace{1cm}

{\bf RAID 3}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{byte} com um disco apenas para paridade. Isto requer que todos os discos operem em \emph{lockstep} (rotação de todos os discos em sincronismo). Assim como RAID-2, tornou-se obsoleta.

\vspace{1cm}

{\bf RAID 4}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um disco apenas para paridade. Se o controlador de disco permitir, um conjunto RAID 4 pode atender várias solicitações de leitura ao mesmo tempo. Todos os \emph{bits} de paridade estão em um único disco, o que pode se tornar um gargalo. RAID-5 substituiu esta configuração.

\vspace{1cm}

{\bf RAID 5}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um único bloco de paridade por \emph{stripe} e os blocos de paridade ficam distribuídos por todos os discos. Esta configuração privilegia a leitura. Uma síndrome é computada para permitir a perda de uma unidade. Essa síndrome P pode ser um simples XOR de dados pelos \emph{stripes}.

\vspace{1cm}

{\bf RAID 6}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com dois blocos de paridade por \emph{stripe} e os blocos de paridade distribuídos por todos os discos. Duas síndromes diferentes precisam ser computadas para permitir a perda de quaisquer duas unidades. Uma delas, P pode ser um simples XOR de dados pelos \emph{stripes}, como em RAID 5. A outra, Q pode ser um XOR de um \emph{linear feedback shift register} de cada \emph{stripe} \cite{Anvin:2009}.

Códigos Low-Density Parity-Check e Tornado são códigos de blocos, lineares e acíclicos. \index{Codificação LPDC}

Código Low-Density Parity-Check (LPDC) é  uma codificação baseada em grafos regulares \cite{Gallager:1963}. Códigos LDPC são conhecidos também como códigos Gallager \cite{LDPCC:2010}. Uma aplicação dessa codificação é a rede \emph{wireless} WMAN WiMAX (IEEE 802.16e \emph{standard for microwave communications}) para internet móvel \cite{wimax:2010}.

%Códigos Tornado são uma classe de códigos LDPC \cite{Woitaszek:2007}. Segundo \cite{Kubiatowicz:2000}, são mais rápidos para codificar e decodificar e necessitam de um pouco mais de $m$ fragmentos para reconstruir a informação. 

Códigos Tornado são uma classe de códigos LDPC (Low Density Parity
Check) que utiliza grafos irregulares e que foi proposta por
M. Luby~\cite{Woitaszek:2007}. Segundo~\cite{Kubiatowicz:2000}, são
mais rápidos para codificar e decodificar e necessitam de um pouco
mais de $k$ fragmentos para reconstruir a informação. Em
\cite{Byers:1998}, o autor comentou o tempo de decodificação para
códigos RS e Tornado. Códigos Tornado usam equações com um número
pequeno de variáveis em contraste com códigos RS. \index{Codificação Tornado}


Em \cite{Luby:1998}, os autores apresentam códigos Tornado baseados em grafos irregulares. Segundo \cite{CS540:2010}, as implicações práticas desses códigos ainda não foram bem estudadas.


\subsubsection{Códigos Convolacionais}

P. Elias introduziu códigos convolucionais em 1955. Um código convulacional é um dispositivo com memória. Apesar de aceitar uma mensagem de entrada de tamanho fixo e produzir uma saída codificada, seus cálculos não dependem somente da entrada atual, mas também das entradas e saídas anteriores.

Um codificador para um código convolucional também aceita blocos de $k$ bits da sequência de dados $u$ e gera uma sequência de saída $v$ de $m$ \emph{bits} chamada \emph{codeword} ou palavra código. Cada bloco da palavra código não depende apenas dos $k$ bits do bloco da sequência de dados correspondente, mas também de $M$ blocos anteriores. Dizemos que o codificador tem memória de ordem $M$, onde $M$ é o número de registradores de memória. Esse conjunto de blocos de $k$ bits, o codificador das palavras código de tamanho $m$ e de memória de ordem $M$ é chamado de ($m$, $k$, $M$) código convolucional. $R = \frac{k}{m}$  é a taxa de codificação. Como o codificador tem memória, ele pode ser implementado como circuito lógico sequêncial \cite{Lin:1983}.

Em um código convolucional, os $m - k$ bits redundantes, que provem à codificação a capacidade de tratar os ruídos do canal, podem ser adicionados quando $k < m$. Para uma mesma taxa de codificação $R$, pode-se adicionar redundância, aumentando a ordem da memória $M$ do codificador. Como usar a memória para obter uma transmissão confiável sob um canal com ruído é o principal problema do projeto do codificador.

Códigos convolucionais podem ser usados para melhorar o desempenho da comunicação por rádio e satélites. Códigos convolucionais são utilizados nas tecnologias CDMA (\emph{Code division multiple access}) e GSM (\emph{Global System for Mobile Communications}) para telefones celulares, \emph{dial-up modems}, satélites, \emph{NASA's Deep Space Network} deep-space communications e na rede WLAN Wi-Fi IEEE 802.11.

