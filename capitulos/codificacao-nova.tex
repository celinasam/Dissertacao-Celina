\chapter{Codificação por Apagamento}

{\small
\begin{align*}
One\ can\ no\ longer\ claim\ to\ be\ a\ communication\ or\\
computer\ system\ engineer\ without\ knowing\ something\ about\ coding.\\
D.\ J.\ Costello,\ J.\ Hagenauer,\ H,\ Imai,\ S.\ B.\ Wicker~\cite{Costello:1998}
\end{align*}
}

A transmissão e o armazenamento de dados tem muito em comum. Ambos transferem dados da fonte para o destino. Para garantir confiabilidade nessas operações, é utilizada codificação por apagamento ou códigos corretores de erros. A Teoria dos Códigos tem sido estudada há décadas: por matemáticos nas décadas de 50 e 60 e a partir da década de 70 por engenheiros \cite{Ritter:2009, Hefez:2008}.

Com a popularização dos computadores e as pesquisas espaciais, os códigos corretores tornaram-se parte comum de comunicações por satélite, de redes de computadores, de armazenamento em discos óticos e outros meios magnéticos. A presença dos códigos corretores de erros é frequente em nosso cotidiano: quando se assiste a um programa de televisão, quando se ouve música a partir de um CD, quando se faz um telefonema, quando se assiste um filme gravado em DVD, quando se navega pela internet.

A codificação de mensagens no emissor antes da transmissão e a
decodificação das mensagens (possivelmente danificadas) que chegam ao
receptor, possibilita reparar os efeitos de um canal físico com ruídos
\cite{Shannon:1948} sem sobrecarregar a taxa de transmissão de
informação ou o \emph{overhead} de armazenamento \cite{Lin:1983}.

% A idéia básica da codificação ótima por apagamento é que o objeto
% original possa ser reconstruído a partir de quaisquer $k$ únicos
% fragmentos que são aproximadamente do mesmo tamanho do objeto original
% \cite{Weatherspoon:2002:01}.
%Para uma codificação quase ótima, são
% necessários $(1+e) \times m$ fragmentos, onde $e \geq 1$
% \cite{EC:2010}.

Um dos principais parâmetros de um código para detecção de erros é a
probabilidade de detecção de erro.

 A probabilidade de erro no canal determina a capacidade de
 transferência de informação no canal. Os modelos estudados por
 pesquisadores envolvem canais simétricos, assimétricos e outros, com
 ou sem memória~\cite{Weber:1985}. 

%Em canais binários simétricos, assume-se que ambos os erros $0 \rightarrow 1$ e $1 \rightarrow 0$ ocorrem com igual probabilidade.

\section{Shannon: conceitos e teoremas fundamentais}

Shannon introduziu dois conceitos fundamentais sobre informação que é transmitida em um sistema de comunicação \cite{Yeung:2008}:

\begin{description}
   \item [a incerteza da informação] se o dado que nos interessa é determinístico, então ele não tem valor algum. Por exemplo, a transmissão contínua de uma imagem em um sistema de televisão é supérflua. Desta forma, a fonte de informação é modelada por uma variável ou processo aleatório e uma probabilidade é utilizada para se desenvolver a teoria da informação.
   \item [a informação transmitida é digital] o dado que nos interessa deve ser convertido em \emph{bits} e ser entregue no destino corretamente, sem referência ao seu significado inicial. O trabalho do Shannon \cite{Shannon:1948} parece ser o primeiro trabalho publicado que usa o termo \emph{bit}.
\end{description}

Nesse mesmo trabalho, Shannon demonstrou dois importantes teoremas que são fundamentais na comunicação ponto-a-ponto:

\begin{description}
   \item [\emph{source coding theorem}] introduz a entropia como medida da informação que, nesse caso, é caracterizada por a taxa mínima de código que representa uma informação livre de erros. Este teorema é a base téorica para compressão de dados.
   \item [\emph{channel coding theorem}] fala da capacidade de um canal com ruídos, na qual a informação é transmitida de forma confiável, desde que ritmo de transferência de dados seja menor que a capacidade do canal. 
\end{description}

A probabilidade de erro no canal determina a capacidade de transferência de informação no canal.

\section{Canais: modelos e erros}

O código da fonte é o conjunto de elementos que definem forma como a informação será transmitida, por exemplo, em termos de \emph{bits} e o código do canal inclui o código da fonte e a redundância (por exemplo, em termos de \emph{bits}) introduzida para garantir a correção de erros.

Uma dificuldade encontrada por quem estuda códigos corretores de erros é que não existe uma nomenclatura unificada \cite{Plank:2009}. Também segundo \cite{CS540:2010}, existem poucos pesquisadores que são programadores de sistemas e que fazem propostas neste tema.

A idéia básica da código ótimo é que o objeto original possa ser reconstruído a partir de quaisquer $k$ únicos fragmentos que são aproximadamente do mesmo tamanho do objeto original \cite{Weatherspoon:2002:01}.
%Para uma codificação quase ótima, são necessários $ \approx  (1+e)k$ fragmentos, onde $e \geq 1$ \cite{EC:2010}.

Corrigir erros é uma tarefa mais complexa que detectá-los. Detectar erros tem a mesma complexidade que a operação de codificar, que pode ser linear no tamanho das palavras código. A operação de decodificar para uma correção de erros ótima é um problema NP-difícil e são conhecidos apenas algoritmos eficientes para algumas classes de códigos. Novas classes de códigos com eficientes decodificadores e novos algoritmos para decodificação para códigos conhecidos são promissoras pesquisas \cite{Klove:2007}.

%Um dos principais parâmetros de um código para detecção de erros é a probabilidade de detecção de erro. Os modelos estudados por pesquisadores envolvem canais simétricos, assimétricos e outros, com ou sem memória.

Em canais binários simétricos (BSC), assume-se que ambos os erros $0 \rightarrow 1$ e $1 \rightarrow 0$  ocorrem com igual probabilidade \cite{Weber:1985}. \index{Canal binário simétrico}

\vspace*{2cm}
\begin{figure}[h]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(0.3,4.1){\vector(1,-1){4}}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(1,3){{\scriptsize $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Canal binário simétrico \cite{Weber:1985}}
   \label{fig0:bsc}
\end{figure}
\vspace*{2cm}


Em algumas aplicações como comunicações óticas, os erros tem uma natureza assimétrica. Canais, onde esse tipo de erro ocorrem, podem ser modelados para canais binários assimétricos ou canal-Z, onde apenas erros com $1$'s ocorrem. Um exemplo disso são sistemas que utilizam \emph{photons} para transmitir informação \cite{Weber:1985}. \index{Canal binário assimétrico}

\vspace*{4cm}
\begin{figure}[h]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Canal binário assimétrico \cite{Weber:1985}}
   \label{fig00:bac}
\end{figure}
\vspace*{2cm}

Existem dois métodos básicos para tratar erros em comunicação e ambos
envolvem a codificação de mensagens. A diferença está em como esses
códigos são utilizados. Em um \emph{Automatic Repeat reQuest}, os códigos
são utilizados para detectar erros e se estes existirem, é feito um
pedido de retransmissão. Com \emph{Forward Error Correction}, os
códigos são usados para detectar e corrigir erros e não é necessário
um caminho de retorno.

\subsection{\emph{Automatic Repeat reQuest} - ARQ}

ARQ utiliza redundância para detectar erros em mensagens e, após a detecção, o destinatário solicita uma repetição da transmissão. Um caminho de retorno é necessário. São sistemas de \emph{two-way transmission}. Exemplos de sistemas que utilizam ARQ são linhas telefônicas e alguns sistemas de satélite \cite{Lin:1983}.

Se existe um grande atraso de propagação, uma grande distância entre emissor e destinatário, este método pode ser muito ineficiente. Podem existir casos em que a retransmissão não é possível, quando não existe \emph{backup}.

Técnicas ARQ incluem repetição seletiva, \emph{Go-back N} e sistemas de reconhecimento positivo ou negativo \cite{Kurose:2010}.

\subsection{\emph{Forward Correction Code} - FEC}

Técnicas FEC representam um \emph{one-way system}. A transmissão ou gravação é restrita a uma direção: da fonte para o sumidouro (destino). Sistemas de armazenamento de fita magnética e sistemas da \emph{NASA's Deep Space Network} utilizam técnicas FEC \cite{Lin:1983}.

O destinatário corrige a mensagem recebida através da codificação por apagamento. Este procedimento geralmente é chamado de correção de erros de repasse e pode ser implementado em \emph{hardware} de propósito especial.

FEC utiliza redundância, assim o decodificador pode corrigir os erros de mensagens no destinatário. Uma analogia pode ser feita com uma pessoa que fala devagar e repetidamente, em uma linha telefônica com ruídos, acrescentando mais redundância para o ouvinte entender a mensagem correta.

\subsubsection{Como FEC funcionam}

Técnicas FEC incluem códigos de blocos (\emph{block codes}), que são códigos sem memória e códigos convolacionais, que são códigos com memória \cite{Berlekamp:1987}. 

\section{Códigos de Blocos}

Na Figura~\ref{fig3:fec}, vemos um sistema que utiliza código de
blocos. A fonte envia uma sequência de dados para o codificador. O
codificador divide esta sequência em blocos de $k$ \emph{bits}
cada chamados mensagens.  Uma mensagem é representada por uma
$k$-tupla binária $u = u_1, u_2,\dots, u_k$. O codificador insere
\emph{bits} redundantes (ou de paridade) para cada mensagem $u$,
gerando uma sequência de saída de $m$ \emph{bits} chamada
\emph{codeword} ou palavra código representada por uma $m$-tupla de
símbolos discretos $v = v_1, v_2, \dots, v_m$.  Os $m - k$ bits são os
bits redundantes que provêm à codificação a capacidade de tratar os
ruídos do canal.

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{center}
  {\begin{picture}(12.5,6)(0,-3)
    \put(0,2){\framebox(3,1){Fonte}}
    \put(3,2.5){\vector(1,0){2}}
    \put(4,2.6){u}
    \put(5,2){\framebox(4,1){{Codificador (m,k)}}}
    \put(9,2.5){\line(1,0){2}}
    \put(10,2.6){v}
    \put(11,2.5){\vector(0,-1){2}}
    \put(9.5,-0.5){\framebox(3,1){Canal}}
    \put(6.8,-0.1){ruídos}
    \put(8,-0){\vector(1,0){1.5}}
    \put(11,-0.5){\line(0,-1){2}}
    \put(11,-2.5){\vector(-1,0){2}}
    \put(10,-2.4){r}
    \put(0,-3){\framebox(3,1){Destino}}
    \put(5,-2.5){\vector(-1,0){2}}
    \put(4,-2.4){$\mathaccent 94 u$}
    \put(5,-3){\framebox(4,1){{Decodificador (m,k)}}}
   \end{picture}}
  \end{center}
  \caption{Códigos de bloco}
  \label{fig3:fec}
\end{figure}
\vspace*{2cm}

Códigos de blocos são identificados pela notação ($m$, $k$), de acordo com o número de \emph{bits} de saída $m$ e o número de \emph{bits} $k$ de cada um dos  blocos de entrada.

Todas as palavras código de um código de blocos tem tamanho fixo, que é um certo número de blocos de  $k$ \emph{bits}. 

A geração de uma palavra código depende apenas de um cálculo algébrico entre os $k$ bits, portanto, um codificador pode ser implementado como um circuito lógico combinacional. O codificador executa o mapeamento: $T :  U \rightarrow V$ onde $U$ é um conjunto de palavras de dados de tamanho $k$ e $V$ é um conjunto de palavras código de tamanho $m$ onde $m > k$. Cada uma das $2^k$ palavras de dados é mapeada para uma única palavra código.

A taxa de codificação e a sobrecarga de armazenamento são
calculados a partir de $k$ blocos originais \cite{RTAD:2007,
  CMSC:2010}. São gerados $m$ símbolos pelo algoritmo de
codificação. $R = \frac{k}{m}$ é a taxa de codificação que pode ser
interpretada como o número de bits de informação por palavra código
transmitida e $O = \frac{1}{R}$ é a sobrecarga de armazenamento.

Se $k \leq m$, mais bits redundantes podem ser adicionados, com aumento do tamanho da palavra código, mantendo $R = \frac{k}{m}$ constante. Como escolher este número $m - k$ de bits redundantes para obter transmissão confiável em cima de um canal com ruídos é o problema principal do projeto do codificador. No destino, o decodificador extrai a sequência original de dados.

Outra métrica utilizada é a redundância que pode ser definida por
$\frac{(m - k)}{m}$. A alta redundância reduz a possibilidade de
todos os dados serem enviados em uma única transmissão. A desvantagem
da redundância é que a adição de \emph{bits} pode exigir uma largura
de banda transmissão maior ou aumentar o atraso das mensagens (ou
ambos).

% Um código C é linear se v e w são palavras código distintas de um código C, então v+w é também uma palavra código de C. Um código linear contém a palavra código zero, pois $v + v = 0$. Operação simples de decodificação, pouca memória e métodos simples para determinação de padrões de erros são algumas das vantagens de códigos lineares.

Um código C é chamado de cíclico se o deslocamento cíclico (\emph{shift}) de qualquer palavra código gera uma nova palavra código. Por exemplo, $C1 = \{000, 110, 101, 011\}$ é um código cíclico. $C2 = \{000, 100, 011, 111\}$ não é um código cíclico.

Existem vários códigos de blocos \cite{Byers:1998, Kubiatowicz:2000}. Alguns dos mais utilizados são códigos Reed-Solomon (RS), códigos Low-Density Parity-Check (LDPC) e códigos Tornado \cite{Mitzenmacher:2004, RTAD:2007}.

%Segundo \cite{Almeida:2007}, códigos Reed-Solomon (RS) são
%particularmente úteis para correção de erros em rajada (seqüência
%símbolos consecutivos, nenhum desses recebidos corretamente, chamados
%\emph{burst errors}). Também podem ser usados eficientemente em canais
%em que o conjunto de símbolos de entrada é consideravelmente grande.


Segundo \cite{AF:2010}, as principais características de códigos de blocos são:

\begin{description}
   \item [taxa de codificação] $R = \frac{k}{m}$ é a medida da eficiência do código, pois é o quociente do número de \emph{bits} da palavra de dados sobre o número de \emph{bits} total da palavra transmitida
   \item [distância mínima] $(d_{min})$ é a menor distância de Hamming \footnote{A distância de Hamming (dH) entre duas palavras código $v_i$ e $v_j$  é o número de \emph{bits} que são diferentes nessas duas palavras.} entre duas quaisquer palavras do código; ela depende do número de \emph{bits} redundantes $q = m - k$, tal que  $(d_{min} \leq q + 1)$
   \item [capacidade de detecção] detecta até $l$ erros, onde $l \leq d_{min} - 1$
   \item [capacidade de correção] corrige os erros até $t$ erros, onde $ t \leq \lfloor \frac{d_{min} - 1}{2} \rfloor$
   \item [capacidade de detecção e correção] detecta até $l$ erros e corrige os erros até $t$ erros, onde $d_{min} \geq l + t + 1$ e $l > t$
\end{description}

Um código com $d_{min} = 1$ não tem capacidade de detectar erros.

\begin{definition} Seja uma sequência de bits $w=w_0w_1 \ldots w_{k-1} \in \{0, 1\}^k$. Podemos definir $w$ como sendo um elemento $w=(w_0w_2 \ldots w_{k-1})$ do produto cartesiano:
\begin{align*}
{\mathbb{Z}_2}^k=\underbrace{\mathbb{Z}_2 \times \ldots \times \mathbb{Z}_2}_{k}
\end{align*}
\end{definition}

\begin{definition} {\bf Peso Hamming} \label{PesoHamm} \index{Peso Hamming}
Seja $u=u_0u_1 \ldots u_k \in {\mathbb{Z}_2}^k$. O peso Hamming  de $u$ é dado por 
\begin{align*}
\omega(u)=|\{i=0,1, \ldots k-1\ : u_i=1\}|
\end{align*}
Em outras palavras, $\omega(u)$ é o número de $1$'s entre os bits de $u$.
\end{definition}

\begin{definition} {\bf Distância Hamming} \label{DistHamm} \index{Distância Hamming}
Seja $u=u_0u_1 \ldots u_k \in {\mathbb{Z}_2}^k$ e $v=v_0v_1 \ldots v_k \in {\mathbb{Z}_2}^k$. A distância entre $u$ e $v$ é dada por $\delta(u,v)=|\{i=0,1, \ldots k-1\ : u_j \neq v_j\}|$
Em outras palavras, a distância Hamming $\delta(u,v)$ é o número de pares de bits correspondentes de $u$ e $v$ em que um bit difere do outro.
\end{definition}

\begin{proposition} Suponha que $u, v \in {\mathbb{Z}_2}^k$. Então $\delta(u,v)=\omega(u \oplus v)$.
\end{proposition}

\begin{proof} Seja $P(n)$ a proposição que $\delta(u,v)=\omega(u \oplus v)$ para $\forall u,v \in {\mathbb{Z}_2}^n$.

Base: $P(1)$ é verdadeira, pois $\delta(0,0)=\omega(0 \oplus 0)=0$, $\delta(0,1)=\omega(0 \oplus 1)=1$, $\delta(1,0)=\omega(1 \oplus 0)=1$ e $\delta(1,1)=\omega(1 \oplus 1)=0$.

Passo de indução: Suponhamos que P(k) é verdadeira $\forall (u,v) \in {\mathbb{Z}_2}^k$ e para $\forall k < n$.

Queremos provar, para $k+1$, que P(k+1) é verdadeira. Seja $u=u_0u_1 \ldots u_k \in {\mathbb{Z}_2}^k$, $v=v_0v_1 \ldots v_k \in {\mathbb{Z}_2}^k$, $u^{'}=u_0u_1 \ldots u_ku_{k+1} \in {\mathbb{Z}_2}^{k+1}$ e $v^{'}=v_0v_1 \ldots v_kv_{k+1} \in {\mathbb{Z}_2}^{k+1}$. Podemos escrever $\delta(u^{'},v^{'})$ como:

\begin{align*}
\delta(u^{'},v^{'})=\delta(u,v)+\delta(u_{k+1},v_{k+1}).
\end{align*}
Sabemos calcular $\delta(u_{k+1},v_{k+1})$, pois $P(1)$ é verdadeira. Pela hipótese de indução, $\delta(u,v)=\omega(u \oplus v)$. Então, $\delta(u^{'},v^{'})=\omega(u,v) \oplus \omega(u_{k+1},v_{k+1})=\omega(u^{'},v^{'})$$\square$
\end{proof}

\begin{proposition} \label{somauns} Suponha que $u, v \in {\mathbb{Z}_2}^k$. Então $\omega(u \oplus v) \leq \omega(u) + \omega(v)$
\end{proposition}

\begin{proof} Seja $P(n)$ a proposição que $\omega(u,v) \leq \omega(u) + \omega(v)$ para $\forall u,v \in {\mathbb{Z}_2}^n$.

Base: $P(2)$ é verdadeira,
$$
\begin{array}{l}
u,v\ \ \ \ u \oplus v\ \ \ \ \omega (u+v)\ \leq \omega(u) + \omega(v)\\
00,00\ \ \ 00\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \  0\\
00,01\ \ \ 01\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 0+1=1\\
00,10\ \ \ 10\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 0+1=1\\
00,11\ \ \ 11\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \ 0\\
01,00\ \ \ 01\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 0+0=0\\
01,01\ \ \ 00\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \ 1+1=2\\
01,10\ \ \ 10\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 1+1=2\\
01,11\ \ \ 10\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 1+0=1\\
10,00\ \ \ 10\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 1+0=1\\
10,01\ \ \ 11\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 1+1=2\\
10,10\ \ \ 00\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \ 1+1=2\\
10,11\ \ \ 01\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 1+0=1\\
11,00\ \ \ 11\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \ 0+0=0\\
11,01\ \ \ 01\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 0+1=1\\
11,10\ \ \ 01\ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ 0+1=1\\
11,11\ \ \ 00\ \ \ \ \ \ 0\ \ \ \ \ \ \ \ \ \ \ \ \ 0+0=0
\end{array}
$$
Passo de indução: Suponhamos que P(k) é verdadeira $\forall (u,v) \in {\mathbb{Z}_2}^k$ e para $\forall k < n$.
Queremos provar, para $k+1$, que P(k+1) é verdadeira. Seja $u=u_0u_1 \ldots u_k \in {\mathbb{Z}_2}^k$, $v=v_0v_1 \ldots v_k \in {\mathbb{Z}_2}^k$ e $z=u \oplus v=z_0z_1 \ldots z_k \in {\mathbb{Z}_2}^k$ e $u^{'}=u_0u_1 \ldots u_ku_{k+1} \in {\mathbb{Z}_2}^k$, $v^{'}=v_0v_1 \ldots v_kv_{k+1} \in {\mathbb{Z}_2}^k$ e $z^{'}=u \oplus v=z_0z_1 \ldots z_kz_{k+1} \in {\mathbb{Z}_2}^{k+1}$. Podemos escrever $\omega (u^{'} \oplus v^{'})=\omega (z^{'})$ como
\begin{align*}
\omega (z^{'})=\omega(z)+\omega(u_{k+1} \oplus v_{k+1})=\omega(u \oplus v)+\omega(u_{k+1} \oplus v_{k+1})
\end{align*}
Sabemos calcular $\omega(u_{k+1} \oplus v_{k+1})$, pois $P(2)$ é verdadeira. Então $\omega(u_{k+1} \oplus v_{k+1}) \leq \omega(u_{k+1})+\omega(v_{k+1})$. Pela hipótese de indução, $\omega(u \oplus v) \leq \omega(u) + \omega(v)$. Então
\begin{align*}
\omega (z^{'}) =\omega(u \oplus v)+\omega(u_{k+1} \oplus v_{k+1}) \leq \omega(u) + \omega(v) + \omega(u_{k+1})+ \omega(v_{k+1})
\end{align*}
$\square$
\end{proof}

\begin{proposition} A distância $\delta : {\mathbb{Z}_2}^k \times {\mathbb{Z}_2}^k \rightarrow {\mathbb{Z}}^*$ tem as seguintes propriedades para $\forall u,v,z \in {\mathbb{Z}_2}^k$:
  \begin{enumerate}[(i)]
     \item $\delta(u,v) \geq 0$
     \item $\delta(u,v)=0,\ se,\ e\ somente\ se,\ u=v$
     \item $\delta(u,v)=\delta(v,u)$
     \item $\delta(u,z) \leq \delta(u,v) + \delta(v,z)$
  \end{enumerate}
\end{proposition}

\begin{proof}
\end{proof}

\begin{definition} {\bf Distância Mínima} \label{DistMin} \index{Distância Mínima} Seja $C:(n,k)$ um código de blocos linear. A  distância mínima
desse código é dada por 
\begin{align*}
d_{min} = min \{ \delta(u,v), u,v \in C:(n,k), u \neq v \}\\
= min= \{ \omega(u), u \in C:(n,k), u \neq 0 \}
\end{align*}
o valor mínimo do peso das palavras código não nulas desse código.

\end{definition}

\begin{definition} Seja $k \in {\mathbb{Z}}^*$ e $u \in {\mathbb{Z}_2}^k$. O conjunto $B(u,k)=\{ v \in {\mathbb{Z}_2}^k : \delta(u,v) \leq k\}$
\end{definition}

\begin{theorem}
Seja $k, m \in {\mathbb{Z}}^*$ e $n > k$, a função de codificação $\alpha\ : {\mathbb{Z}_2}^k \rightarrow {\mathbb{Z}_2}^m$, $\zeta=\alpha({\mathbb{Z}_2}^k)$ e $\tau(c)=\ sequ\hat{e}ncia\ de\ bits\ de\ c\ ap\acute{o}s\ a\ leitura\ ou\ a\ transmiss\tilde{a}o\ de\ c$
  \begin{enumerate}[(i)]
     \item Suponhamos que $\delta(c,d) > k$ para $\forall c, d \in \zeta, c \neq d$. Então a transmissão ou o armazenamento de $\delta(c,\tau(c)) \leq k$ pode ser sempre detectada, ou seja, até $k$ erros podem ser detectados;
     \item Suponhamos que $\delta(u,v) > 2k$ para $\forall c, d \in \zeta, c \neq d$. Então a transmissão ou o armazenamento de $\delta(c,\tau(c)) \leq k$ pode ser sempre detectada e corrigida, ou seja, até $2k$ erros podem ser detectados e corrigidos.
  \end{enumerate}
\end{theorem}

\begin{proof} $(i)$ Já que $\delta(c,d) > k$  para $\forall c, d \in \zeta, c \neq d$, então $\forall c \in \zeta$, $B(c,k) \cap \zeta = {c}$. Assim, com a transmissão ou a leitura de no mínimo 1 e no máximo k erros de $c$, temos que $\tau(c) \neq c$ e $\tau(c) \in B(c,k)$. Logo, $\tau(c) \not\in \zeta$.

$(ii)$ Como (i) é verdadeira, $\forall c, d \in \zeta, d \neq c$, temos que, pela proposição~\ref{somauns}, $2k < \delta(c,d) \leq \delta(c,\tau(c)) + \delta(\tau(c),d)$. Sabemos que $\delta(c,\tau(c)) \leq k$ e que $\delta(d,\tau(c)) > k$, de modo que $\tau(c) \not\in B(c,k)$. Por isso sabemos exatamente qual elemento de $\zeta$ que originou $\tau(c)$.
\end{proof}

\begin{theorem} Seja $C:(n,k)$ um código de blocos linear e sua matriz de verificação de paridade ${\bf H}$. Para cada vetor código $u \in C:(n,k)$ e $l$ igual do seu peso $\omega(u)$, $\exists l$ colunas da matriz ${\bf H}$ tais que o vetor soma dessas $l$ colunas é igual ao vetor $0$. Da mesma forma, se o vetor soma de $l$ colunas da matriz ${\bf H}$ for o vetor $0$, então $\exists v \in C:(n,k)$ tal que $\omega(v)=l$.
\end{theorem}

\begin{proof}
\end{proof}

\begin{corollary} Seja $C:(n,k)$ um código de blocos linear e sua matriz de verificação de paridade ${\bf H}$. O peso mínimo ou a distância mínima de $C:(n,k)$ é igual ao menor número de colunas de ${\bf H}$ cuja soma é o vetor $0$.
\end{corollary}


Códigos Reed-Solomon são códigos de blocos, lineares e cíclicos. São códigos parametrizáveis, cuja capacidade de correção de erros pode ser alterada facilmente. \index{Codificação Reed-Solomon}

%Definition. A Reed-Solomon (or RS) code over GF(q) is a BCH (BOSE-CHAUDHURI-HOCQUENGHEM) code of length N = q - 1. Of course q is never 2. Thus the length is the number of nonzero elements in the ground field. We shall use N , K and D to denote the length, dimension, and minimum distance (using capital letters to distinguish them from the parameters of the binary codes which will be constructed later). RS codes are important in concatenated codes and burst correction.

%Em \cite{Plank:1997}, o autor apresenta uma especificação completa do problema e do algoritmo da codificação e detalhes de sua implementação. O modelo estudado é formado por $n$ dispositivos de armazenamento  $D_1, D_2, ..., D_n$ (\emph{data devices}) e outros $m$ dispositivos de armazenamento  $C_1, C_2, ..., C_m$ (\emph{checksum devices}). O conteúdo de cada um dos $m$ \emph{checksum devices} é calculado a partir do conteúdo dos $n$ \emph{data devices}. O objetivo do cálculo dos $C_i$ para $1 \leq i \leq m$ é tal que para quaisquer $m$ dispositivos que falhem dos $D_1, D_2, ..., D_n, C_1, C_2, ..., C_m$, o conteúdo dos dispositivos que falharam possa ser reconstituído a partir dos dispositivos que não falharam.

Segundo \cite{Almeida:2007}, códigos RS são particularmente úteis para correção de erros em rajada (seqüência símbolos consecutivos, nenhum desses recebidos corretamente, chamados \emph{burst errors}). Também podem ser usados eficientemente em canais onde o conjunto de símbolos de entrada é consideravelmente grande.

%Uma implementação de biblioteca em C/C++ para o algoritmo RS foi apresentada em \cite{Plank:2007}.

O sistema de armazenamento OceanStore \cite{Kubiatowicz:2000} e o protocolo BitTorrent (aplicação da camada de rede da internet) usam uma codificação RS.

\emph{Redundant Arrays of Inexpensive [Independent] Disks} (RAID) é uma classe de códigos RS. RAID é um método para prover tolerância a falhas ou alto desempenho em sistemas de armazenagem utilizando para isso uma codificação de correção de erros ou paridade. RAID foi introduzido por D. A. Patterson na Universidade da California, Berkeley (UC Berkeley) em 1988 \cite{Patterson:1988}. \index{RAID}

Segundo \cite{Woitaszek:2007}, para sistemas de armazenamento, a
codificação por apagamento baseada em operações simples, tais como XOR
RAID, são preferíveis. Embora um mecanismo externo deva ser utilizado
para detectar erros, as operações de XOR podem ser realizadas
rapidamente e resultar em alto \emph{throughput} das operações de
codificação e decodificação.

São conceitos básicos \cite{Vadala:2002}:

\begin{description}

   \item [data striping] é uma técnica para segmentar dados sequênciais, como um arquivo, de maneira que o acesso a segmentos sequênciais seja feito por diferentes dispositivos de armazenamento. Esta técnica é útil quando se quer processar mais rapidamente os pedidos de acesso a dados que os dispositivos de armazenamento permitem. Diferentes segmentos de dados são mantidos em diferentes dispositivos de armazenamento. A falha de um dos dispositivos torna toda a sequência de dados indisponível. Essa desvantagem é superada pelo armazenamento de informações redundantes (custo de armazenamento extra), como a paridade, com o objetivo de correção de erros. As configurações de RAID que utilizam paridade são RAID-2, RAID-3, RAID-4, RAID-5 e RAID-6 \cite{DS:2010}.

   \item [stripe] são segmentos consecutivos ou faixas que são escritos sequencialmente através de cada um dos discos de um \emph{array} ou conjunto. Cada segmento tem um tamanho definido em blocos.

\end{description}

\vspace{1cm}

{\bf RAID 2}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{bit} e usa códigos Hamming para correção de erros. Por exemplo, o código Hamming(7,4) (quatro \emph{bits} de dados e tres \emph{bits} de paridade) permite usar 7 discos em RAID 2, sendo 4 usados para armazenar dados e 3 usados para correção de erros. Esta codificação tornou-se padrão para \emph{hard drives} e tornou-se desnecessária, assim deixou de ser vantajosa.

\vspace{1cm}

{\bf RAID 3}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{byte} com um disco apenas para paridade. Isto requer que todos os discos operem em \emph{lockstep} (rotação de todos os discos em sincronismo). Assim como RAID-2, tornou-se obsoleta.

\vspace{1cm}

{\bf RAID 4}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um disco apenas para paridade. Se o controlador de disco permitir, um conjunto RAID 4 pode atender várias solicitações de leitura ao mesmo tempo. Todos os \emph{bits} de paridade estão em um único disco, o que pode se tornar um gargalo. RAID-5 substituiu esta configuração.

\vspace{1cm}

{\bf RAID 5}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um único bloco de paridade por \emph{stripe} e os blocos de paridade ficam distribuídos por todos os discos. Esta configuração privilegia a leitura. Uma síndrome é computada para permitir a perda de uma unidade. Essa síndrome P pode ser um simples XOR de dados pelos \emph{stripes}.

\vspace{1cm}

{\bf RAID 6}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com dois blocos de paridade por \emph{stripe} e os blocos de paridade distribuídos por todos os discos. Duas síndromes diferentes precisam ser computadas para permitir a perda de quaisquer duas unidades. Uma delas, P pode ser um simples XOR de dados pelos \emph{stripes}, como em RAID 5. A outra, Q pode ser um XOR de um \emph{linear feedback shift register} de cada \emph{stripe} \cite{Anvin:2009}.

Códigos Low-Density Parity-Check e Tornado são códigos de blocos, lineares e acíclicos. \index{Codificação LPDC}

Código Low-Density Parity-Check (LPDC) é  uma codificação baseada em grafos regulares \cite{Gallager:1963}. Códigos LDPC são conhecidos também como códigos Gallager \cite{LDPCC:2010}. Uma aplicação dessa codificação é a rede \emph{wireless} WMAN WiMAX (IEEE 802.16e \emph{standard for microwave communications}) para internet móvel \cite{wimax:2010}.

%Códigos Tornado são uma classe de códigos LDPC \cite{Woitaszek:2007}. Segundo \cite{Kubiatowicz:2000}, são mais rápidos para codificar e decodificar e necessitam de um pouco mais de $m$ fragmentos para reconstruir a informação. 

Códigos Tornado são uma classe de códigos LDPC (Low Density Parity
Check) que utiliza grafos irregulares e que foi proposta por
M. Luby~\cite{Woitaszek:2007}. Segundo~\cite{Kubiatowicz:2000}, são
mais rápidos para codificar e decodificar e necessitam de um pouco
mais de $k$ fragmentos para reconstruir a informação. Em
\cite{Byers:1998}, o autor comentou o tempo de decodificação para
códigos RS e Tornado. Códigos Tornado usam equações com um número
pequeno de variáveis em contraste com códigos RS. \index{Codificação Tornado}


Em \cite{Luby:1998}, os autores apresentam códigos Tornado baseados em grafos irregulares. Segundo \cite{CS540:2010}, as implicações práticas desses códigos ainda não foram bem estudadas.

\subsection{Capacidade de Correção de Erros}

Após a transmissão ou armazenamento de um vetor código $v$, um padrão de $l$ erros resultará em um vetor recebido ou lido $r$ que difere do vetor $v$ em $l$ bits, ou seja, $\delta(v,r)=l$. Se a distância mínima de $C:(n,k)$ é $d_{min}$, nenhum padrão de erro de $d_{min}-1$ ou menos pode trocar um vetor código por outro. Ou seja, todo padrão de erro de $d_{min}-1$ ou menos irá resultar em um vetor $r$ que não é um vetor código de $C$. Quando o receptor detecta que o vetor recebido não é um vetor código de $C$, os erros são detectados. Por esse motivo, um código de bloco com distância mínima $d_{min}$ é capaz de detectar todos os padrões de erro com $d_{min}-1$ ou menos. Esse mesmo código não é capaz de detectar todos os padrões de erro com $d_{min}$ ou mais, pois existe pelo menos um par de vetores código que diferem de $d_{min}$ bits.

\begin{proposition} Seja um código de bloco $C:(n,k)$. O código $C$ é capaz de detectar $2^n - 2^k$ padrões de erro de tamanho $n$.
\end{proposition}

\begin{proof} Existem $2^n -1$ padrões de erro não nulos e desses $2^k-1$ padrões de erro são idênticos aos $2^k-1$ vetores código não nulos. Se um desses padrões ocorre, um vetor código $v$ é recebido ou lido como $w$. A síndrome será igual a $0$. O decodificador aceita $w$ e confirma uma decodificação incorreta. Portanto existem $2^k-1$ padrões de erro não detectáveis. Assim, existem $2^n -1 - (2^k-1)=2^n - 2^k$ padrões de erro detectáveis, isso inclue o vetor $0$.
\end{proof}

Podemos acrescentar que um código de bloco $C:(n,k)$ corrige $2^{n-k}$ padrões de erro de tamanho $n$.

\begin{definition} {\bf Distribuição Peso} \label{DistPeso} \index{Distribuição Peso} Seja um código de bloco $C:(n,k)$. Seja $A_i$ o número de vetores código de $C$ com peso $i$. A sequência $A_o, A_1, \ldots A_n$ é a distribuição peso de $C$. No caso de canal binário simétrico, a probabilidade que o decodificador falhe em detectar erros pode ser expressada:
\begin{align*}
P_u(E)=\displaystyle\sum_{i=1}^{n} A_i p^i(1 -p)^{n-i}
\end{align*}
onde $p$ é probabilidade da ocorrência de um erro no canal. Se a distância mínima de $C$ é $d_{min}$, $A_1=A_{d_{min}-1} =0$.
\end{definition}

\begin{example} Para o código $CH(7,4)$ do exemplo~\ref{CodHamm}, a distribuição peso é $A_0=1, A_1=A_2=0, A_3=7, A_5=A_6=0$ e $A_7=1$. A probabilidade de não detectar erros é:
\begin{align*}
P_u(E)=0+0+7p^3(1-p)^4+7p^4(1-p)^3+0+0+p^7=7p^3(1-p)^4+7p^4(1-p)^3+p^7
\end{align*}
Mesmo para $p \ll 1$, $p=10^{-2}$, $P_u(E) \approx 7 \times 10^{-6}$. Esse resultado pode ser assim interpretado: a cada 1 milhão de palavras código, 7 erros não são detectados pelo decodificador do código $CH(7,4)$. Devido a sua simplicidade, os códigos Hamming são usados em memórias RAM, onde a taxa de erros é baixa e são conhecidos pelo nome SECDED (\emph{Single Error Correction, Double Error Detection}).
\end{example}

Vamos determinar a capacidade de correção de erros de um código de bloco linear $C:(n,k)$. 

\begin{proposition} {\bf Capacidade de correção de erros de um código de bloco linear $C:(n,k)$} \label{CapCorrErros} \index{Capacidade de Correção de erros de um código de bloco linear $C:(n,k)$} Seja $C:(n,k)$ um código de bloco linear. Seja $t$ o número de bits que pode ser corrigido e $d_{min}$ a distância mínima de $C$. Como $d_{min}$ é um inteiro, vamos escrever $t$ na seguinte condição:
\begin{align*}
2t+1 \leq d_{min} \leq 2t + 2
\end{align*}
Então, $C$ é capaz de detectar todos os padrões de $t$ erros. 
\end{proposition}

\begin{proof} Seja $v$ o vetor código transmitido ou armazenado e $r$ o vetor recebido ou lido. Seja $w \in C$. As distâncias hamming dos vetores satisfazem a inequação:
\begin{align*}
\delta(v,r)+\delta(w,r) \geq \delta(v,w)
\end{align*}
Suponhamos que ocorram $t^{'}$ erros na transmissão ou armazenagem de $v$, então $\delta(v,r)=t^{'}$. Como $v, w \in C$, $\delta(v,w) \geq d_{min} \geq 2t+1$. Assim, reescrevendo a inequação:
\begin{align*}
\delta(v,r)+\delta(w,r) \geq \delta(v,w)\\
t^{'}+\delta(w,r) \geq 2t+1\\
\delta(w,r) \geq 2t+1 - t^{'}
\end{align*}
Se $t^{'} \leq t$, então
\begin{align*}
\delta(w,r) > t
\end{align*}
Isso mostra que na ocorrência de de $t$ erros ou menos, o vetor $r$ é muito próximo do vetor $v$ que qualquer outra palavra $w$ no código $C$.

O código $C$ não é capaz de corrigir todos os padrões de erros com $l > t$.
Um código de bloco linear com distância mínima $d_{min}$ garante corrigir todos os padrões de erros $t=\lfloor \frac{(d_{min}-1)}{2} \rfloor$ ou menos. O parâmetro $t$ é chamado capacidade de correção de erros aleatórios de um código.
\end{proof}

Se um código de bloco linear $C:(n,k)$, capaz de corrigir todos os padrões de erro de peso $t$ ou menor, é usado na transmissão ou armazenamento em um canal binário simétrico com probabilidade de erro $p$, a probabilidade do decodificador falhe em detectar erros tem limite superior dado por:
\begin{align*}
P_u(E)=\displaystyle\sum_{i=t+1}^{n} \binom {n}{i} p^i(1 -p)^{n-i}
\end{align*}

Esses códigos são usados de tal forma que os padrões de erro de peso $\lambda$ são corrigidos e os padrões de erro de peso $l > \lambda$ são detectados. Isto é possível se $d_{min} \geq l + \lambda + 1$.

\begin{theorem} {\bf Limite de Singleton} \label{LimSingle} \index{Limite de Singleton} A distância mínima de um código de blocos $C:(n,k)$ tem  limite superior dado por $d_{min} \leq n - k + 1$. 
\end{theorem}

\begin{proof}  O vetor código não nulo com menor peso tem peso $d_{min}$, por definição. Existem vetores código com apenas um símbolo de informação não-nulo + $(n - k)$ símbolos de paridade. Tal vetor código não pode ter peso maior que $1+(n-k)$. Logo, o peso mínimo do código - isto é, $d_{min}$, não
pode ser maior do que $1 + (n - k)$.
\end{proof}

Códigos de bloco que alcançam o limite de Singleton são chamados códigos MDS. Com códigos binários, a igualdade só se atinge com códigos de repetição. Nesse caso, a sobrecarga de armazenamento é grande ($k = 1$, $O = n/k = n$).

\subsection{Matriz Padrão e Detecção de Síndrome}

\begin{theorem} {\bf Limite de Hamming} \label{LimHamm} \index{Limite de Hamming}  O número de bits de verificação de paridade de qualquer código linear binário (n,k) com distância mínima $dmin \geq 2t + 1$ satisfaz o limite de Hamming
\begin{align*}
2^{n-k} \geq  1 + \binom {n}{1} + \binom {n}{2} + \ldots + \binom {n}{t}=\displaystyle\sum_{i=0}^{t} \binom {n}{t}
\end{align*}
\end{theorem}

\begin{proof}  Pela proposição~\ref{CapCorrErros}, $d_{min} \geq 2t + 1$, então todos os vetores código de peso $t$ ou menos (isto é, padrões de $t$ erros ou menos) podem ser usados como \emph{coset leaders} de uma matriz padrão. Se o código corrige $t$ ou menos erros por vetor código, o conjunto de todos os padrões de $t$ ou menos erros (incluindo o padrão nulo)
\begin{align*}
\binom{n}{0} + \binom {n}{1} + \binom {n}{2} + \ldots + \binom {n}{t}
\end{align*}
deve ser menor ou igual ao número de \emph{cosets leaders} $2^{n-k}$.
\end{proof}

O limite de Hamming estabelece um valor máximo para a capacidade de correção de erros $t$ de um código. A igualdade verifica-se nos códigos perfeitos, onde não há nenhum vetor código de $n$ bits a uma distância maior que $t$ de outro vetor código.

Temos um limitante menor para a capacidade de correção de erros de um código, o limite de Plotkin.

\begin{theorem} {\bf Limite de Plotkin} \label{LimPlot} \index{Limite de Plotkin} Se dispormos em as $2k$ vetores código de um código $C(n,k)$, obteremos uma matriz de $2^k$ linhas e $n$ colunas. Em cada uma dessas colunas, metade dos bits (isto é, $\frac{2^k}{2}$) é $0$ e a outra metade é $1$. Logo, o número total de $1$'s que existem em todos os vetores código é
\begin{align*}
n\frac{2^k}{2}=n2^{k-1}
\end{align*}
Como existem $2^k-1$ vetores não nulos em $C(n,k)$, o peso médio dos vetores código é
\begin{align*}
\frac{n2^{k-1}}{2^k-1}
\end{align*}
Assim,
\begin{align*}
d_{min} \leq \frac{n2^{k-1}}{2^k-1}
\end{align*}
ou
\begin{align*}
\frac{d_{min}}{n} \leq \frac{2^{k-1}}{2^k-1}
\end{align*}
\end{theorem}

\subsection{Exemplos de Códigos de Blocos}

\begin{definition} Seja uma sequência de bits $w=w_0w_1 \ldots w_{m-1} \in \{0, 1\}^m$. Podemos definir $w$ como sendo um elemento $w=(w_0w_2 \ldots w_{m-1})$ do produto cartesiano:
\begin{align*}
{\mathbb{Z}_2}^m=\underbrace{\mathbb{Z}_2 \times \ldots \times \mathbb{Z}_2}_{m}
\end{align*}
\end{definition}

\begin{example} Considere o código de bloco (6,5). A função de codificação $\alpha\ :\ {\mathbb{Z}_2}^5 \rightarrow {\mathbb{Z}_2}^6$ é definida da seguinte forma: para cada $w \in {\mathbb{Z}_2}^5$, seja $c=\alpha(w)=w_0w_1 \ldots w_4w_5 \in {\mathbb{Z}_2}^6$ e $w5=w_0+w_1+ \ldots +w_4\ (mod\ 2)$. Cada $c \in {\mathbb{Z}_2}^6$, o número de bits $1$ é sempre par. Desta forma, um erro (de transmissão ou de armazenamento) será detectado, mas sem a possibilidade de correção.
\end{example}

\begin{example} Considere o código de bloco (21,7). A função de codificação $\alpha\ :\ {\mathbb{Z}_2}^7 \rightarrow {\mathbb{Z}_2}^{21}$ é definida da seguinte forma: para cada $w=w_0w_1 \ldots w_7 \in {\mathbb{Z}_2}^7$, seja $c=\alpha(w)=w_0w_1 \ldots w_7w_0w_1 \ldots w_7w_0w_1 \ldots w_7$, ou seja, repetimos mais 2 vezes a mesma sequência. Depois da transmissão ou do armazenamento, suponhamos que a sequência

\begin{align*}
\tau(c)=v_0v_1 \ldots v_7{v_0}^{'}{v_1}^{'} \ldots {v_7}^{'} \ldots v_7{v_0}^{''}{v_1}^{''} \ldots {v_7}^{''}
\end{align*}
é recebida ou lida. Vamos definir a função de decodificação $\sigma\ :\ {\mathbb{Z}_2}^{21} \rightarrow {\mathbb{Z}_2}^7$ dessa forma:

\begin{align*}
\sigma(v_0v_1 \ldots v_7{v_0}^{'}{v_1}^{'} \ldots {v_7}^{'} \ldots v_7{v_0}^{''}{v_1}^{''} \ldots {v_7}^{''})=u_0u_1 \ldots u_7
\end{align*}

onde cada $u_j$ é igual ao valor de maior ocorrência entre $v_j$, ${v_j}^{'}$ e ${v_j}^{''}$. Nesse caso, se no máximo um bit entre $v_j$, ${v_j}^{'}$ e ${v_j}^{''}$ for diferente de $w_j$, então ainda teremos $u_j=w_j$. Assim, nesse caso, até um erro para cada $w_j$ pode ser detectado e corrigido.

\end{example}


\section{Códigos Convolacionais}

P. Elias introduziu códigos convolucionais em 1955. Um código convulacional é um dispositivo com memória. Apesar de aceitar uma mensagem de entrada de tamanho fixo e produzir uma saída codificada, seus cálculos não dependem somente da entrada atual, mas também das entradas e saídas anteriores.

Um codificador para um código convolucional também aceita blocos de $k$ bits da sequência de dados $u$ e gera uma sequência de saída $v$ de $m$ \emph{bits} chamada \emph{codeword} ou palavra código. Cada bloco da palavra código não depende apenas dos $k$ bits do bloco da sequência de dados correspondente, mas também de $M$ blocos anteriores. Dizemos que o codificador tem memória de ordem $M$, onde $M$ é o número de registradores de memória. Esse conjunto de blocos de $k$ bits, o codificador das palavras código de tamanho $m$ e de memória de ordem $M$ é chamado de ($m$, $k$, $M$) código convolucional. $R = \frac{k}{m}$  é a taxa de codificação. Como o codificador tem memória, ele pode ser implementado como circuito lógico sequêncial \cite{Lin:1983}.

Em um código convolucional, os $m - k$ bits redundantes, que provem à codificação a capacidade de tratar os ruídos do canal, podem ser adicionados quando $k < m$. Para uma mesma taxa de codificação $R$, pode-se adicionar redundância, aumentando a ordem da memória $M$ do codificador. Como usar a memória para obter uma transmissão confiável sob um canal com ruído é o principal problema do projeto do codificador.

Códigos convolucionais podem ser usados para melhorar o desempenho da comunicação por rádio e satélites. Códigos convolucionais são utilizados nas tecnologias CDMA (\emph{Code division multiple access}) e GSM (\emph{Global System for Mobile Communications}) para telefones celulares, \emph{dial-up modems}, satélites, \emph{NASA's Deep Space Network} deep-space communications e na rede WLAN Wi-Fi IEEE 802.11.


