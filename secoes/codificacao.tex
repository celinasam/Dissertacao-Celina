\section{Códigos Corretores de Erros}

A transmissão e o armazenamento de informação tem muito em comum. Ambos transferem dados da fonte para o destino. Para garantir confiabilidade nessas operações, são utilizados códigos corretores de erros.

Em um trabalho publicado em 1948, o matemático Claude E. Shannon demonstrou que a eficiência da codificação de mensagens do emissor antes da transmissão e da decodificação das mensagens (possivelmente danificadas) que chegam no receptor, possibilita reparar, para um nível aceitável, os efeitos de um canal físico com ruídos \cite{Shannon:1948} sem sobrecarregar a taxa de transmissão de informação ou o \emph{overhead} de armazenamento \cite{Lin:1983}. A Teoria dos Códigos tem sido estudada há décadas: por matemáticos nas décadas de 50 e 60 e a partir da década de 70 por engenheiros \cite{Hefez:2008}.

Com a popularização dos computadores e as pesquisas espaciais, os códigos corretores tornaram-se parte comum de comunicações por satélite, de redes de computadores, de armazenamento em discos óticos e outros meios magnéticos. A presença dos códigos corretores de erros é frequente em nosso cotidiano: quando se assiste a um programa de televisão, quando se ouve música a partir de um CD, quando se faz um telefonema, quando se assiste um filme gravado em DVD, quando se navega pela internet.

\subsection{Shannon: conceitos e teoremas fundamentais}

Shannon introduziu dois conceitos fundamentais sobre informação que é transmitida em um sistema de comunicação \cite{Yeung:2008}:

\begin{description}
   \item [a incerteza da informação] se o dado que nos interessa é determinístico, então ele não tem valor algum. Por exemplo, a transmissão contínua de uma imagem em um sistema de televisão é supérflua. Desta forma, a fonte de informação é modelada por uma variável ou processo aleatório e uma probabilidade é utilizada para se desenvolver a teoria da informação.
   \item [a informação transmitida é digital] o dado que nos interessa deve ser convertido em \emph{bits} e ser entregue no destino corretamente, sem referência ao seu significado inicial. Esse trabalho do Shannon parece ser o primeiro trabalho publicado que usa o termo \emph{bit} (\emph{binary digit}).
\end{description}

Nesse mesmo trabalho, Shannon demonstrou dois importantes teoremas que são fundamentais na comunicação ponto-a-ponto:

\begin{description}
   \item [\emph{source coding theorem}] introduz a entropia como medida da informação que, nesse caso, é caracterizada por a taxa mínima de código que representa uma informação livre de erros. Este teorema é a base téorica para compressão de dados.
   \item [\emph{channel coding theorem}] fala da capacidade de um canal com ruídos, na qual a informação é transmitida de forma confiável, desde que ritmo de transferência de dados seja menor que a capacidade do canal. 
\end{description}

A probabilidade de erro no canal determina a capacidade de transferência de informação no canal.

\subsection{Canais: modelos e erros}

O código da fonte é o conjunto de elementos que definem forma como a informação será transmitida, por exemplo, em termos de \emph{bits} e o código do canal inclui o código da fonte e a redundância (por exemplo, em termos de \emph{bits}) introduzida para garantir a correção de erros.

Uma dificuldade encontrada por quem estuda códigos corretores é que não existe uma nomenclatura unificada \cite{Plank:2009}. Também segundo \cite{CS540:2010}, existem poucos pesquisadores que são programadores de sistemas e que fazem propostas neste tema.

A idéia básica da código ótimo é que o objeto original possa ser reconstruído a partir de quaisquer $m$ únicos fragmentos que são aproximadamente do mesmo tamanho do objeto original \cite{Weatherspoon:2002:01}. Para uma codificação quase ótima, são necessários $ \approx  (1+e)m$ fragmentos, onde $e \geq 1$ \cite{EC:2010}.

Corrigir erros é uma tarefa mais complexa que detectá-los. Detectar erros tem a mesma complexidade que a operação de codificar, que pode ser linear no tamanho das palavras código. A operação de decodificar para uma correção de erros ótima é um problema NP-difícil e são conhecidos apenas algoritmos eficientes para algumas classes de códigos. Novas classes de códigos com eficientes decodificadores e novos algoritmos para decodificação para códigos conhecidos são promissoras pesquisas \cite{Klove:2007}.

Um dos principais parâmetros de um código para detecção de erros é a probabilidade de detecção de erro. Os modelos estudados por pesquisadores envolvem canais simétricos, assimétricos e outros, com ou sem memória.

Em canais binários simétricos, assume-se que ambos os erros $0 \rightarrow 1$ e $1 \rightarrow 0$  ocorrem com igual probabilidade \cite{Weber:1985}.

\vspace{2cm}

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(0.3,4.1){\vector(1,-1){4}}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(1,3){{\scriptsize $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Figura ~\ref{fig0:bsc} - Canal binário simétrico \cite{Weber:1985}}
   \label{fig0:bsc}
\end{figure}


   \vspace{2cm}

Em algumas aplicações como comunicações óticas, os erros tem uma natureza assimétrica. Canais, onde esse tipo de erro ocorrem, podem ser modelados para canais binários assimétricos ou canal-Z, onde apenas erros com $1$'s ocorrem. Um exemplo disso são sistemas que utilizam \emph{photons} para transmitir informação \cite{Weber:1985}.

\vspace{2cm}

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{center}
  \begin{picture}(3,3)
    \put(0,4){0}
    \put(0.3,4.1){\vector(1,0){4}}
    \put(4.4,4){0}
    \put(2,4.2){{\scriptsize 1 - $\rho$}}
    \put(0.3,0.1){\vector(1,1){4}}
    \put(0,0){1}
    \put(0.3,0.1){\vector(1,0){4}}
    \put(4.4,0){1}
    \put(2,0.2){{\scriptsize 1 - $\rho$}}
    \put(1,1.2){{\scriptsize $\rho$}} 
    \put(0,-2){{\scriptsize $\rho$ = probabilidade da ocorrência de um erro}}
   \end{picture}
   \end{center}
   \caption{Figura ~\ref{fig00:bac} - Canal binário assimétrico \cite{Weber:1985}}
   \label{fig00:bac}
\end{figure}



   \vspace{2cm}

\subsection{Transmissão e armazenamento de informação}

A transmissão e o armazenamento de informação tem muito em comum. Ambos transferem dados da fonte para o destino.

\vspace{2cm}

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{picture}(4,3)
    \put(0,2){\framebox(3,1){{\scriptsize Information source}}}
    \put(3,2.5){\vector(1,0){1}}
    \put(4,2){\framebox(3,1){{\scriptsize Source encoder}}}
    \put(7,2.5){\vector(1,0){1}}
    \put(7.5,2.6){u}
    \put(8,2){\framebox(3,1){{\scriptsize Channel encoder}}}
    \put(11,2.5){\vector(1,0){1}}
    \put(11.2,2.6){v}
    \put(12,2){\framebox(3,1){{\tiny Modulator (writing unit)}}}
    \put(13.5,2){\vector(0,-1){1.5}}
    \put(12,-0.5){\framebox(3,1){{\scriptsize Coding channel}}}
    \put(9.5,-0.1){{\scriptsize ruídos}}
    \put(10.5,-0){\vector(1,0){1.5}}
    \put(13.5,-0.5){\vector(0,-1){1.5}}
    \put(12,-3){\framebox(3,1){{\tiny Modulator (reading unit)}}}
    \put(12,-2.5){\vector(-1,0){1}}
    \put(11.2,-2.4){r}
    \put(8,-2.5){\vector(-1,0){1}}
    \put(7.5,-2.4){$\mathaccent 94 u$}
    \put(8,-3){\framebox(3,1){{\scriptsize Channel decoder}}}
    \put(4,-3){\framebox(3,1){{\scriptsize Source decoder}}}
    \put(4,-2.5){\vector(-1,0){1}}
    \put(0,-3){\framebox(3,1){{\scriptsize Destination}}}
   \end{picture}
   \caption{Figura ~\ref{fig1:dtss} - Diagrama de uma transmissão de dados ou um armazenamento de informação \cite{Lin:1983}}
   \label{fig1:dtss}
   \vspace{4cm}
\end{figure}
  

   \vspace{2cm}

Na figura ~\ref{fig1:dtss}, o \emph{source encoder} transforma a saída da fonte em uma sequência de bits $u$. Esse codificador é projetado para que o número de bits que represente a saída da fonte seja mínimo e para que a saída da fonte possa ser reconstruída sem ambiguidade.

O \emph{channel encoder} transforma a sequência $u$ em uma sequência codificada discreta $v$, também chamada \emph{codeword}. As instâncias de $v$ também são sequências binárias e permitem códigos não-binários.

O \emph{modulator} transforma cada símbolo da sequência $v$ em uma onda de duração de $T$ segundos que pode ser transmitida ou gravada. Esta onda entra em um canal ou mídia de armazenamento e é corrompida pelo ruído. Os canais de transmissão podem ser linhas telefônicas, \emph{links} de rádio, de telemetria, de satélite. Mídias de armazenamento podem ser memórias, fitas magnéticas, discos magnéticos, unidades de memória ótica. Cada um desses canais ou mídias podem ser expostos a vários ruídos.

O \emph{demodulator} recebe a onda de duração de $T$ segundos e produz uma saída que pode ser discreta ou contínua. A sequência de saída do \emph{demodulator} corresponde a sequência codificada $v$ e é chamada de sequência recebida $r$.

O \emph{channel decoder} transforma a sequência recebida $r$ em uma sequência binária $\mathaccent 94 u$ chamada de sequência estimada. Idealmente a sequência  $\mathaccent 94 u$ será uma réplica da sequência $u$ original.

O \emph{source decoder} transforma a sequência estimada $\mathaccent 94 u$ em uma sequência o mais próxima possível da saída da fonte e a entrega para o sumidouro.

Em um diagrama mais simplificado (figura ~\ref{fig2:mcs}), estão enfatizadas as operações de codificação e decodificação. A \emph{information source} e o \emph{source encoder} são combinados em uma fonte digital com saída $u$. O \emph{modulator}, o \emph{channel encoder} e o \emph{demodulator} são combinados em um \emph{coding channel} com entrada $v$ e saída $r$.  O \emph{source decoder} e o \emph{destination} são combinados em um sumidouro digital.

\vspace{2cm}

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{picture}(3,3)
    \put(0,2){\framebox(3,1){Fonte digital}}
    \put(3,2.5){\vector(1,0){2}}
    \put(4,2.6){u}
    \put(5,2){\framebox(3,1){Codificador}}
    \put(8,2.5){\line(1,0){2}}
    \put(9,2.6){v}
    \put(10,2.5){\vector(0,-1){2}}
    \put(8.5,-0.5){\framebox(3,1){Coding channel}}
    \put(5.8,-0.1){ruídos}
    \put(7,-0){\vector(1,0){1.5}}
    \put(10,-0.5){\line(0,-1){2}}
    \put(10,-2.5){\vector(-1,0){2}}
    \put(9,-2.4){r}
    \put(0,-3){\framebox(3,1){{\scriptsize Sumidouro digital}}}
    \put(5,-2.5){\vector(-1,0){2}}
    \put(4,-2.4){$\mathaccent 94 u$}
    \put(5,-3){\framebox(3,1){Decodificador}}
   \end{picture}
   \caption{Figura ~\ref{fig2:mcs} - Modelo simplificado de um sistema de codificação \cite{Lin:1983}}
   \label{fig2:mcs}
\end{figure}

 
   \vspace{2cm}


Em \cite{Huffman:2003} são apresentados o processo de codificação e decodificação de uma mensagem utilizando o teorema de Shannon.

\subsection{Técnicas para detecção e correção de erros}

Existem dois métodos básicos para tratar erros em comunicação e ambos envolvem a codificação de mensagens. A diferença está em como esses códigos são utilizados. Em um \emph{repeat request system}, os códigos são utilizados para detectar erros e se existirem erros, é feito um pedido de retransmissão. Com \emph{forward erros correction}, os códigos são usados para detectar e corrigir erros.

Os erros de um canal com ruídos podem ser corrigidos por um das seguintes estratégias: \emph{Automatic Repeat reQuest} e \emph{Forward corrEction Code}  \cite{Purser:1995}.

\subsubsection{\emph{Automatic Repeat reQuest} - ARQ}

ARQ utiliza redundância para detectar erros em mensagens e, após a detecção, o destinatário solicita uma repetição da transmissão. Um caminho de retorno é necessário. São sistemas de \emph{two-way transmission}. Exemplos de sistemas que utilizam ARQ são linhas telefônicas e alguns sistemas de satélite \cite{Lin:1983}.

Se existe um grande atraso de propagação, uma grande distância entre emissor e destinatário, este método pode ser muito ineficiente. Podem existir casos em que a retransmissão não é possível, quando não existe \emph{backup}.

Técnicas ARQ incluem repetição seletiva, \emph{Go-back N} e sistemas de reconhecimento positivo ou negativo \cite{Kurose:2010}.

\subsubsection{\emph{Forward Correction Code} - FEC}

O diagrama da figura ~\ref{fig1:dtss} representa um \emph{one-way system}. A tranmissão ou gravação é restrita a uma direção: da fonte para o sumidouro (destino). Sistemas de armazenamento de fita magnética e sistemas  da \emph{NASA's Deep Space Network} utilizam técnicas FEC \cite{Lin:1983}.

O destinatário corrige a mensagem recebida através de \emph{Error correcting-codes}. Este procedimento geralmente é chamado de correção de erros de repasse e pode ser implementado em \emph{hardware} de propósito especial.

FEC utiliza redundância, assim o decodificador pode corrigir os erros de mensagens no destinatário. Não é necessário um caminho de retorno.

Uma analogia pode ser feita com uma pessoa que fala devagar e repetidamente, em uma linha telefônica com ruídos, acrescentando mais redundância para o ouvinte entender a mensagem correta.


\subsubsection{Como FEC funcionam}
Os tipos de códigos de canal podem ser agrupados em duas grandes categorias: códigos de bloco e ódigos convolacionais.

Técnicas FEC incluem códigos convolacionais, que são códigos com memória e códigos de blocos (\emph{block codes}, que são códigos sem memória \cite{Berlekamp:1987}. O alfabeto dos códigos explicados aqui consistem de dois elementos: $0$ e $1$. O conjunto $\{0, 1\}$ é denominado $K$.

\vspace{1cm}

{\bf Códigos de Blocos}

\begin{figure}[htb]
  \setlength{\unitlength}{1cm}
  \begin{picture}(3,3)
    \put(0,2){\framebox(3,1){Fonte}}
    \put(3,2.5){\vector(1,0){2}}
    \put(4,2.6){u}
    \put(5,2){\framebox(3,1){{\scriptsize Codificador (n,k)}}}
    \put(8,2.5){\line(1,0){2}}
    \put(9,2.6){v}
    \put(10,2.5){\vector(0,-1){2}}
    \put(8.5,-0.5){\framebox(3,1){Coding channel}}
    \put(5.8,-0.1){ruídos}
    \put(7,-0){\vector(1,0){1.5}}
    \put(10,-0.5){\line(0,-1){2}}
    \put(10,-2.5){\vector(-1,0){2}}
    \put(9,-2.4){r}
    \put(0,-3){\framebox(3,1){Destino}}
    \put(5,-2.5){\vector(-1,0){2}}
    \put(4,-2.4){$\mathaccent 94 u$}
    \put(5,-3){\framebox(3,1){{\scriptsize Decodificador (n,k)}}}
   \end{picture}
   \caption{Figura ~\ref{fig3:fec} - Como códigos de blocos funcionam}
   \label{fig3:fec}
\end{figure}

   \vspace{2cm}

Na figura ~\ref{fig3:fec}, vemos um sistema que utiliza código de blocos. A fonte envia uma sequência de dados para o codificador. O codificador divide a sequência de dados em $m$ blocos de  $k$ \emph{bits} cada chamados mensagens.  Uma mensagem é representada por uma $k$-tupla binária $u = u_1, u_2, ... , u_k$. Existem $2^k$ diferentes mensagens. O codificador insere \emph{bits} redundantes (ou de paridade) para cada mensagem $u$, gerando uma sequência de saída de $n$ \emph{bits} chamada \emph{codeword} ou palavra código representada por uma $n$-tupla de símbolos discretos $v = v_1, v_2, ... , v_n$.  Fazendo uma correspondência entre as $2^k$ mensagens, existem $2^k$ diferentes palavras código para a saída do codificador. Esse conjunto de $2^k$ palavras código de tamanho $n$ é chamado de códigos de blocos ($n$, $k$).

Códigos de blocos são identificados pela notação ($n$, $k$), de acordo com o número de \emph{bits} de saída $n$ e o número de \emph{bits} $k$ de cada um dos  blocos de entrada. Os $n - k$ bits são os bits redundantes que provem à codificação a capacidade de tratar os ruídos do canal.

Todas as palavras código de um código de blocos tem tamanho fixo, que é um certo número de blocos de  $k$ \emph{bits}. 

A geração de uma palavra código depende apenas de um cálculo algébrico entre os $k$ bits, portanto, um codificador pode ser implementado como um circuito lógico combinacional.

O codificador executa o mapeamento: $T :  U \rightarrow V$ onde $U$ é um conjunto de palavras de dados de tamanho $k$ e $V$ é um conjunto de palavras código de tamanho $n$ onde $n > k$. Cada uma das $2^k$ palavras de dados é mapeada para uma única palavra código.

A taxa de codificação e o \emph{overhead} de armazenamento são calculados a partir de $m$ blocos originais  \cite{RTAD:2007} \cite{CMSC:2010}. São gerados $n$ símbolos pelo algoritmo de codificação. $R = \frac{k}{n}$  é a taxa de codificação que pode ser interpretada como o número de bits de informação por palavra código transmitida e $O = \frac{1}{R}$ é o \emph{overhead} de armazenamento.

Se $k \leq n$, mais bits redundantes podem ser adicionados, com aumento do tamanho da palavra código, mantendo $R = \frac{k}{n}$ constante. Como escolher este número $n - k$ de bits redundantes para obter transmissão confiável em cima de um canal com ruídos é o problema principal do projeto do codificador.

No destino, o decodificador extrai a sequência original de dados. O algoritmo de decodificação é mais complexo que o da codificação.

Outra métrica utilizada é a redundância que pode ser definida por $\frac{(n - k)}{n}$. A codificação que introduz uma alta redundância, isto é, $(n - k) \gg$ ou $(\frac{k}{n}) \ll$, transmite menos informação por palavra codificada. A codificação que introduz menos redundância transmite mais informação por palavra codificada.

A alta redundância é vantajosa porque reduz a possibilidade de todos os dados serem enviados em uma única transmissão. A desvantagem da redundância é que a adição de \emph{bits} pode exigir uma largura de banda transmissão maior ou aumentar o atraso das mensagens (ou ambos).

Se o sistema não exige uma transferência de dados em tempo-real, então o atraso das mensagens é um usual \emph{trade-off}.

A figura ~\ref{fig4:cbc} apresenta uma classificação para códigos de blocos.

    \begin{figure}[h]
      \centering
      \includegraphics[scale=1]{figuras/blockcodes.png}
      \caption{Uma classificação para códigos de blocos \cite{MathWorks:2010}}
      \label{fig4:cbc}
    \end{figure} 

\vspace{2cm}

Um código C é linear se v e w são palavras código distintas de um código C, então v+w é também uma palavra código de C. Um código linear contém a palavra código zero, pois $v + v = 0$. Operação simples de decodificação, pouca memória e métodos simples para determinação de padrões de erros são algumas das vantagens de códigos lineares.

Um código C é chamado de cíclico se o deslocamento cíclico (\emph{shift}) de qualquer palavra código gera uma nova palavra código. Por exemplo, $C1 = \{000, 110, 101, 011\}$   é um código cíclico. $C2 = \{000, 100, 011, 111\}$ não é um código cíclico.

\subsection{Códigos Convolacionais}

P. Elias introduziu códigos convolucionais em 1955. Um código convulacional é um dispositivo com memória. Apesar de aceitar uma mensagem de entrada de tamanho fixo e produzir uma saída codificada, seus cálculos não dependem somente da entrada atual, mas também das entradas e saídas anteriores.

Um codificador para um código convolucional também aceita blocos de $k$ bits da sequência de dados $u$ e gera uma sequência de saída $v$ de $n$ \emph{bits} chamada \emph{codeword} ou palavra código. Cada bloco da palavra código não depende apenas dos $k$ bits do bloco da sequência de dados correspondente, mas também de $M$ blocos anteriores. Dizemos que o codificador tem memória de ordem $M$, onde $M$ é o número de registradores de memória. Esse conjunto de blocos de $k$ bits, o codificador das palavras código de tamanho $n$ e de memória de ordem $M$ é chamado de ($n$, $k$, $M$) código convolucional. $R = \frac{k}{n}$  é a taxa de codificação. Como o codificador tem memória, ele pode ser implementado como circuito lógico sequêncial \cite{Lin:1983}.

Em um código convolucional, os $n - k$ bits redundantes, que provem à codificação a capacidade de tratar os ruídos do canal, podem ser adicionados quando $k < n$. Para uma mesma taxa de codificação $R$, pode-se adicionar redundância, aumentando a ordem da memória $M$ do codificador. Como usar a memória para obter uma transmissão confiável sob um canal com ruído é o principal problema do projeto do codificador.

Códigos convolucionais podem ser usados para melhorar o desempenho da comunicação por rádio e satélites. Códigos convolucionais são utilizados nas tecnologias CDMA (\emph{Code division multiple access}) e GSM (\emph{Global System for Mobile Communications}) para telefones celulares, \emph{dial-up modems}, satélites, \emph{NASA's Deep Space Network} deep-space communications e na rede WLAN Wi-Fi IEEE 802.11.

\vspace{1cm}

Existem vários algoritmos de codificação por apagamento \cite{Byers:1998, Kubiatowicz:2000}. Alguns dos mais utilizados são códigos Reed-Solomon (RS), códigos Low-Density Parity-Check (LDPC) e códigos Tornado \cite{Mitzenmacher:2004} \cite{RTAD:2007}.

Segundo \cite{Woitaszek:2007}, para sistemas de armazenamento, a codificação por apagamento baseada em operações simples, tais como XOR RAID e códigos Tornado,  são preferíveis. Apesar de que um mecanismo externo deva ser utilizado para detectar erros, as operações de XOR podem ser realizadas rapidamente e resultar em alto \emph{throughput} das operações de codificação e decodificação.

\subsection{Características de Códigos de Blocos}


A distância de Hamming (dH) entre duas palavras código $v_i$ e $v_j$  é o número de \emph{bits} que são diferentes nessas duas palavras.

Segundo \cite{AF:2010}, as principais características de códigos de blocos são:

\begin{description}
   \item [taxa de codificação] $R = \frac{k}{n}$ é a medida da eficiência do código, pois é o quociente do número de \emph{bits} da palavra de dados sobre o número de \emph{bits} total da palavra transmitida
   \item [distância mínima] $(d_{min})$ é a menor distância de Hamming entre duas quaisquer palavras do código; ela depende do número de \emph{bits} redundantes $q = n - k$, tal que  $(d_{min} \leq q + 1)$
   \item [capacidade de detecção] detecta até $l$ erros, onde $l \leq d_{min} - 1$
   \item [capacidade de correção] corrige os erros até $t$ erros, onde $ t \leq \lfloor \frac{d_{min} - 1}{2} \rfloor$
   \item [capacidade de detecção e correção] detecta até $l$ erros e corrige os erros até $t$ erros, onde $d_{min} \geq l + t + 1$ e $l > t$
\end{description}

Um código com $d_{min} = 1$ não tem capacidade de detectar erros.

\subsection{Códigos de Blocos Lineares}

\subsubsection{Códigos Reed-Solomon}

São códigos de blocos, lineares e cíclicos.

Definition. A Reed-Solomon (or RS) code over GF(q) is a BCH (BOSE-CHAUDHURI-HOCQUENGHEM) code of length N = q - 1. Of course q is never 2. Thus the length is the number of nonzero elements in the ground field. We shall use N , K and D to denote the length, dimension, and minimum distance (using capital letters to distinguish them from the parameters of the binary codes which will be constructed later). RS codes are important in concatenated codes and burst correction.

Em \cite{Plank:1997}, o autor apresenta uma especificação completa do problema e do algoritmo da codificação e detalhes de sua implementação. O modelo estudado é formado por $n$ dispositivos de armazenamento  $D_1, D_2, ..., D_n$ (\emph{data devices}) e outros $m$ dispositivos de armazenamento  $C_1, C_2, ..., C_m$ (\emph{checksum devices}). O conteúdo de cada um dos $m$ \emph{checksum devices} é calculado a partir do conteúdo dos $n$ \emph{data devices}. O objetivo do cálculo dos $C_i$ para $1 \leq i \leq m$ é tal que para quaisquer $m$ dispositivos que falhem dos $D_1, D_2, ..., D_n, C_1, C_2, ..., C_m$, o conteúdo dos dispositivos que falharam possa ser reconstituído a partir dos dispositivos que não falharam.

Segundo \cite{Almeida:2007}, códigos RS são particularmente úteis para correção de erros em rajada (seqüência símbolos consecutivos, nenhum desses recebidos corretamente, chamados \emph{burst errors}). Também podem ser usados eficientemente em canais onde o conjunto de símbolos de entrada é consideravelmente grande.

Uma implementação de biblioteca em C/C++ para o algoritmo RS foi apresentada em \cite{Plank:2007}.

O sistema de armazenamento OceanStore \cite{Kubiatowicz:2000} e o protocolo BitTorrent (aplicação da camada de rede da internet) usam uma codificação RS.

\subsubsection{Códigos RAID}

\emph{Redundant Arrays of Inexpensive [Independent] Disks} (RAID) é uma classe de códigos RS. RAID é um método para prover tolerância a falhas ou alto desempenho em sistemas de armazenagem utilizando para isso uma codificação de correção de erros ou paridade. RAID foi introduzido por D. A. Patterson na Universidade da California, Berkeley (UC Berkeley) em 1988 \cite{Patterson:1988}.

São conceitos básicos \cite{Vadala:2002}:

\begin{description}

   \item [data striping] é uma técnica para segmentar dados sequênciais, como um arquivo, de maneira que o acesso a segmentos sequênciais seja feito por diferentes dispositivos de armazenamento. Esta técnica é útil quando se quer processar mais rapidamente os pedidos de acesso a dados que os dispositivos de armazenamento permitem. Diferentes segmentos de dados são mantidos em diferentes dispositivos de armazenamento. A falha de um dos dispositivos torna toda a sequência de dados indisponível. Essa desvantagem é superada pelo armazenamento de informações redundantes (custo de armazenamento extra), como a paridade, com o objetivo de correção de erros. As configurações de RAID que utilizam paridade são RAID-2, RAID-3, RAID-4, RAID-5 e RAID-6 \cite{DS:2010}.

   \item [stripe] são segmentos consecutivos ou faixas que são escritos sequencialmente através de cada um dos discos de um \emph{array} ou conjunto. Cada segmento tem um tamanho definido em blocos.

\end{description}

\vspace{1cm}

{\bf RAID 2}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{bit} e usa códigos Hamming para correção de erros. Por exemplo, o código Hamming(7,4) (quatro \emph{bits} de dados e tres \emph{bits} de paridade) permite usar 7 discos em RAID 2, sendo 4 usados para armazenar dados e 3 usados para correção de erros. Esta codificação tornou-se padrão para \emph{hard drives} e tornou-se desnecessária, assim deixou de ser vantajosa.

\vspace{1cm}

{\bf RAID 3}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de \emph{byte} com um disco apenas para paridade. Isto requer que todos os discos operem em \emph{lockstep} (rotação de todos os discos em sincronismo). Assim como RAID-2, tornou-se obsoleta.

\vspace{1cm}

{\bf RAID 4}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um disco apenas para paridade. Se o controlador de disco permitir, um conjunto RAID 4 pode atender várias solicitações de leitura ao mesmo tempo. Todos os \emph{bits} de paridade estão em um único disco, o que pode se tornar um gargalo. RAID-5 substituiu esta configuração.

\vspace{1cm}

{\bf RAID 5}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com um único bloco de paridade por \emph{stripe} e os blocos de paridade ficam distribuídos por todos os discos. Esta configuração privilegia a leitura. Uma síndrome é computada para permitir a perda de uma unidade. Essa síndrome P pode ser um simples XOR de dados pelos \emph{stripes}.

\vspace{1cm}

{\bf RAID 6}

\vspace{0.5cm}

Esta configuração divide os dados a nivel de bloco com dois blocos de paridade por \emph{stripe} e os blocos de paridade distribuídos por todos os discos. Duas síndromes diferentes precisam ser computadas para permitir a perda de quaisquer duas unidades. Uma delas, P pode ser um simples XOR de dados pelos \emph{stripes}, como em RAID 5. A outra, Q pode ser um XOR de um \emph{linear feedback shift register} de cada \emph{stripe}.

Passo 0: nó 0 cria palavra código em blocos b1, b2, b3, ..., bk

Passo 1: nó 0 cria b1 e o envia para os nó 1 e nó 2

Passo 2: nó 0 cria b2 e o envia para os nó 1 e nó 3

Passo 3: nó 0 cria b3 e o envia para os nó 1 e nó 4

Passo 4: nó 0 e nó 1 codificam b1, b2 e b3 e criam o bloco de paridade p1 = f1(b1, b2, b3) e p2 = f2(b1, b2, b3) 

\vspace{1cm}

\subsubsection{Códigos Low-Density Parity-Check e Tornado}

São códigos de blocos, lineares e acíclicos.

Código Low-Density Parity-Check (LPDC) é  uma codificação baseada em grafos regulares \cite{Gallager:1963}. Códigos LDPC são conhecidos também como códigos Gallager \cite{LDPCC:2010}. Uma aplicação dessa codificação é a rede \emph{wireless} WMAN WiMAX (IEEE 802.16e \emph{standard for microwave communications}) para internet móvel \cite{wimax:2010}.

Códigos Tornado são uma classe de códigos LDPC \cite{Woitaszek:2007}. Segundo \cite{Kubiatowicz:2000}, são mais rápidos para codificar e decodificar e necessitam de um pouco mais de $m$ fragmentos para reconstruir a informação. 

Em \cite{Luby:1998}, os autores apresentam códigos Tornado baseados em grafos irregulares. Segundo \cite{CS540:2010}, as implicações práticas desses códigos ainda não foram bem estudadas.

%Na tabela~\ref{tab1:comp}, alguns sistemas que utilizam com
%codificação por apagamento foram comparados.

%\input{tabelas/tabela-comp-sistemas-arquivos-codificacao}

%\subsection{Replicação versus Codificação por Apagamento}

%Para implementar redundância de dados em sistemas, são utilizadas várias técnicas: codificação por apagamento, replicação, espelhamento, \emph{Cyclic redundancy check} (CRC), \emph{bits} de paridade, \emph{checksum}, assinatura digital \cite{EDC:2010}. Mecanismos de redundância podem implementar um conjunto destas técnicas \cite{Fan:2009}.

%Entre as técnicas para implementar redundância de dados, a replicação é a mais utilizada em sistemas com o objetivo de se obter alta disponibilidade e durabilidade de dados diante de falhas de componentes destes sistemas. O motivo disto é a simplicidade na implementação de replicação, mas algumas pesquisas demonstram um campo promissor para codificação por apagamento para armazenamento em sistemas distribuídos.

%A principal desvantagem da replicação é que ela requer um grande \emph{overhead} de armazenamento para pouco ganho em disponibilidade e tolerância a falhas. Garantir que os dados permaneçam disponíveis quando todos os $n$ dispositivos falham exige que, pelo menos, $n + 1$ cópias existam \cite{Woitaszek:2007}. Por exemplo, o sistema Glaciar de armazenamento aumenta de 11 vezes a quantidade de dados armazenados utilizando replicação para conseguir 0.999999\% (\emph{six nines})  de confiabilidade.

%Uma vantagem de codificação por apagamento seria um custo menor de armazenamento se comparado a replicação, no caso de grande volume de dados. Outra vantagem com relação a replicação foi comentada em \cite{Weatherspoon:2002:01}: para um mesmo espaço de armazenamento, o tempo médio entre falhas (\emph{mean time to failure}) é maior.

%Os autores em \cite{Dabek:2004} afirmam que dados replicados permitem leituras de baixa latência, porque há muitas opções para a seleção de servidores, enquanto que dados codificados reduzem o consumo de largura de banda para escritas, em detrimento do aumento da latência de leituras.

