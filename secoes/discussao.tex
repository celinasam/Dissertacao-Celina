\section{Discussão sobre esquemas adequados de redundância de dados}

Pode-se encontrar na literatura um número significativo de projetos que propõem sistemas de armazenamento distribuído, sistemas de arquivos distribuídos, ou sistemas de backup. Apesar da disso, nenhum esquema de redundância de dados tem sido amplamente aceito para esses sistemas e nenhuma regra fácil foi criada para se encontrar um esquema adequado de redundância de dados.

Os autores em \cite{Duminoco:2009} estudaram redundância de dados em sistemas \emph{peer-to-peer} para \emph{backup} e propuseram um esquema hídrido para implementar redundância (replicação e codificação) de dados. Em \cite{Storer:2008}, foi proposto um sistema de armazenamento de dados baseado em discos que usa dois níveis de codificação. Nesses textos, encontramos uma comparação entre alguns sistemas de armazenamento, avaliando-se algumas de suas características. Podemos observar que os tipos de esquema de redundância de dados são replicação, codificação por apagamento e híbrido, sendo a replicação, a estratégia mais utilizada nos sistemas comparados por \cite{Duminoco:2009} e a codificação por apagamento, para os sistemas comparados por \cite{Storer:2008}.

Redundância de dados é necessária para prevenir perda de dados, mas não é suficiente. A avaliação de esquemas de redundância é muitas vezes baseada na suposição de que as réplicas falham de forma independente. Na prática, as falhas não são tão independentes, segundo \cite{Weatherspoon:2002:02,Baker:2006}. Esse trabalho não tratará a independência das réplicas.

Cada esquema de redundância estabelece (i) como criar os dados redundantes e (ii) como reconstruir os dados quando houver falha. Essas duas operações geram custos que diferem de um esquema para outro. Esse trabalho comentará os mais amplamente usados esquemas de redundância: replicação e códigos corretores de erros.

\subsection{Esquemas de redundância de dados para sistema de armazenamento}

Esquemas de redundância de dados são utilizados em sistemas de armazenamento (exemplo: sistemas RAID) para prover disponibilidade, tolerância a falhas e durabilidade de dados e em sistemas de comunicação (exemplo, sistemas \emph{peer-to-peer}) para prover uma entrega confiável e segura de dados.

\subsubsection{Replicação}

Replicação é o esquema de redundância mais simples. A maioria dos sistemas, que utiliza redundância de dados, é baseada em replicação, mas esse esquema consume mais espaço que a codificação por apagamento, pois uma cópia completa de cada arquivo é armazenada em cada um dos servidores de dados.

A principal desvantagem da replicação é que ela requer um grande \emph{overhead} de armazenamento para pouco ganho em disponibilidade e tolerância a falhas. Garantir que os dados permaneçam disponíveis quando todos os $n$ dispositivos falham exige que, pelo menos, $n + 1$ cópias existam \cite{Woitaszek:2007}. Por exemplo, o sistema Glaciar de armazenamento aumenta de 11 vezes a quantidade de dados armazenados utilizando replicação para conseguir 0.999999\% (\emph{six nines}) de confiabilidade.

Os autores em \cite{Dabek:2004} afirmam que dados replicados permitem leituras de baixa latência, porque há muitas opções para a seleção de servidores, enquanto que dados codificados reduzem o consumo de largura de banda para escritas, em detrimento do aumento da latência de leituras.

A replicação é usada no Google File System \cite{Ghemawat:2003} (GFS), no Hadoop Distributed File System \cite{Hadoop:2010} (HDFS) e no Kosmos distributed file system \cite{TKDFS:2011} (KFS), sistemas de arquivos distribuídos que apresentam características semelhantes. Um \emph{cluster} do GFS ou do HDFS ou do KFS é formado por um único servidor, master (GFS) ou namenode (HDFS) ou Meta server (KFS), que mantém os metadados e muitos servidores de dados, os chunkservers (GFS e KFS) ou os datanodes (HDFS) e é acessado por vários clientes. Os arquivos de dados são armazenados nos chunkservers (GFS e KFS) ou datanodes (HDFS) e são particionados em blocos de igual tamanho. GFS, HDFS e KFS foram projetados para aplicações que processam grande volume de dados. 

Seus projetos consideram \emph{clusters} de (\emph{commodity hardware}), uma versão do \emph{kernel} linux como sistema operacional para as máquinas e uma arquitetura de rede com dois níveis: vários \emph{racks} interligados por um comutador e cada \emph{rack} é formado por várias máquinas e seus discos, estes também interligados por um comutador. A estratégia de inserção de dados cria réplicas em \emph{racks} distintos do \emph{rack} onde está a 1$^a$ réplica, assim, falhas que comprometam um \emph{rack} não provocam a indisponibilidade de dados. Os arquivos de dados são alterados por concatenações ao invés de sobrescrever dados existentes. Após a criação, os arquivos de dados são usados apenas para leitura e esta leitura ocorre sequencialmente. O KFS permite escrever em posições randômicas nos arquivos. As APIs do cliente fornecidas pelo GFS, pelo HDFS e pelo KFS suportam operações de criação, leitura, escrita, remoção de arquivos, mas não implementam a interface POSIX.

O GFS está disponível para linux sob uma licença de software proprietário. O HDFS \footnote{http://hadoop.apache.org/}  e o KFS \footnote{http://code.google.com/p/kosmosfs/} estão disponíveis para linux sob uma licença Apache. 

O Farsite, que utiliza apenas replicação, é um sistema de arquivos distribuídos, particionados em namespaces, explorando os desktops presentes dentro da Microsoft, sem servidor mestre, disponível para Windows sob uma licença de software proprietário. A escolha da replicação foi, pelos autores, considerada uma opção mais simples para disponibilidade, já que a codificação poderia significar latência adicional nas leituras dos arquivos. Ainda segundo os autores, estudos com experimentos já mostraram que a codificação pode apresentar um bom desempenho e seria possível, então, alterar no futuro o esquema de redundância do Farsite.

Big Table (construído sob o GFS) e Dynamo (construído para Amazon.com) são dois sistemas de armazenamento que gravam e recuperaram dados através de uma chave e executam em um \emph{pool} compartilhado de máquinas, utilizam apenas replicação. 

Ceph \cite{Weil:2006} é um sistema de arquivos \emph{open source} que possui três principais componentes: um \emph{cluster} de servidores de metadados (que gerencia o namespace, nomes de arquivos e diretórios), um \emph{cluster} de OSDs (dispositivos de armazenamento de objetos) que armazenam dados e metadados e os clientes que utilizam uma interface do sistema de arquivos. O Ceph agrupa dados em PGs (grupos de colocação) e usa uma função \emph{hash} para distribuir os PGs nos OSDs, cujo algoritmo CRUSH é $O(log n)$ e usa uma árvore-B para indexar os PGs. Existe um módulo em desenvolvimento que permite usar o Ceph como armazenamento para uma instância do Hadoop. O Ceph utiliza apenas replicação, implementa parcialmente a interface POSIX e está disponível para linux sob LGPL \footnote{http://ceph.newdream.net/}.

Lustre \footnote{git://git.lustre.org/prime/lustre.git} tem na sua arquitetura os metadata server (disponibiliza os metadados para clientes), o metadata target (um por sistema de arquivos, armazena os metadados), object storage servers (armazena os dados), object storage target (armazena os objetos que contém os arquivos de dados) e clientes.

Moosefs \footnote{http://www.moosefs.org/} também foi projetado com uma arquitetura que se assemelha a do GFS, HDFS e KFS: master server (que armazena os metadados), chunk servers (que armazenam os dados), metalogger server (podem substituir algumas funções do master server, se ele falhar) e clientes (que solicitam dados e se comunicam com o master server e o chunk servers. 

Ambos, Lustre e Moosefs, usam replicação, implementam a interface POSIX e estão disponíveis para linux sob uma licença GPL.

Com exceção do Farsite, os sistemas de armazenamento apresentados consideram \emph{clusters} de (\emph{commodity hardware}) e uma versão do \emph{kernel} linux como sistema operacional para as máquinas.

\subsubsection{Codificação por Apagamento}

Uma vantagem de codificação por apagamento é um custo menor de armazenamento se comparado a replicação, no caso de grande volume de dados. Outra vantagem com relação a replicação foi comentada em \cite{Weatherspoon:2002:01}: para um mesmo espaço de armazenamento, o tempo médio entre falhas (\emph{mean time to failure}) é maior.

\begin{table}
\singlespacing
  \begin{center}
    \begin{tabular}{|p{2cm}||p{4cm}||p{5cm}||p{2cm}|}
      \hline \hline
sistema & codificação &  arquitetura do sistema & licença\\ \hline \hline
Pergamum \cite{Storer:2008}& XOR parity, Reed-Solomon &  disk-based archival storage & não disponível\\ \hline
Potshards & RAID e Reed-Solomon &  disk-based archival storage & não disponível\\ \hline
RobuStore & Luby Transform (LT) codes & disk-based archival storage & não disponível\\ \hline
Glacier \cite{Haeberlen:2005}& Reed-Solomon com matrizes Cauchy & peer-to-peer storage system & não disponível\\ \hline
Total Recall & Maymounkov's online codes &  peer-to-peer storage system & não disponível\\ \hline
FAB \cite{Saito:2004}& Reed-Solomon & distributed disk array & não disponível\\ \hline
GPFS & RAID & shared-disk file system for cluster & não disponível\\ \hline
Oceanstore \cite{Kubiatowicz:2000} \footnote{http://oceanstore.sourceforge.net/}& Tornado codes e Read-Solomon codes & desktops e notebooks conectados a servidores geograficamente distribuídos & BSD \\ \hline
xFS \footnote{http://oss.sgi.com/projects/xfs/} & RAID & serverless network file system & GPL\\ \hline
Swift & RAID & desktops sob unix conectados a uma intranet & não disponível\\ \hline
Tahoe-LAFS \footnote{http://tahoe-lafs.org/trac/tahoe-lafs} & RAID & peer-to-peer filesystem & GPL2\\ \hline
HDFS \cite{Hadoop:2010}& RAID, Reed-Solomon, ... & shared-disk file system for cluster & Apache\\ \hline
    \end{tabular}
\caption{Comparação entre sistemas de armazenamento de grande volume de dados que utilizam \emph{commodity hardware}}
\label{tab1:comp}
  \end{center}
\end{table}

HDFS, Total Recall e OceanStore usam codificação para reduzir o tamanho do armazenamento de dados e todos os outros sistemas usam codificação para prover disponibilidade e confiabilidade.

Esses sistemas que foram comparados, estão disponíveis para uma versão de sistema operacional linux ou unix. Além da codificação, todos eles implementam replicação.

Vamos avaliar algumas das métricas utilizadas em literatura para comparar redundância de dados em sistemas de armazenamento: sobrecarga de armazenamento, disponibilidade dos \emph{peers} e operações de criação, leitura, atualização consistente e remoção de dados redundantes. Inicialmente vamos definir alguns conceitos adaptados de \cite{Duminoco:2009, Chiola:2005} para esses dois esquemas de redundância.

\subsection{Caracterização da replicação e da codificação}

A confiabilidade de um esquema de redundância é medida pelo número de falhas simultâneas que ele pode tolerar sem comprometer a capacidade de reconstruir os dados originais. Assim esta propriedade pode ser expressada como a {\bf probabilidade de perda de dados, dado que ocorreram $l$ falhas}, $P(l)$.

Para avaliar o armazenamento, vamos definir {\bf fator de redundância} $B$, uma razão entre tamanho dos dados originais mais a redundância $|dado\ +\ red|$ e o tamanho dos dados originais $|dado|$ e também vamos definir grau de reparação.

O {\bf grau de reparação} mede o que deve ser feito após parte da redundância ser perdida. Para isso, é feita uma leitura de dados disponíveis para produzir novos. O custo dessa leitura em um sistema de armazenamento distribuído inclue volume do tráfego da rede, política de reparação, algoritmo de coordenação. Nesse estudo vamos apenas avaliar a contribuição do esquema de redundância: a quantidade de dados a serem lidos para que dados novos sejam criados para reparar o dado corrompido, definido por $d$.

Definimos $p$ como a probabilidade de um \emph{peer} estar disponível e  $q\ =\ 1\ -\ p$ como a probabilidade de um \emph{peer} não estar disponível. Nesse estudo, assumimos que elas são iguais para todos os \emph{peers}.

Vamos definir as operações de acesso a dados redundantes como uma tupla $(r, w, a)$, onde $r$ é o número mínimo de \emph{peers} disponíveis que armazenam dados redundantes, $w$ é o número mínimo de \emph{peers} disponíveis que armazenam dados redundantes que devem ser acessados para armazenar novos valores e $a$ número de \emph{peers} (provavelmente outros) que devem estar disponíveis para completar a operação.

\subsubsection{Replicação}

Para as definições, $n$ é o número de réplicas dos dados.

{\bf número de falhas suportadas} $l\ =\ n\ -\ 1$

{\bf probabilidade de perda de dados, dado que ocorreram $l$ falhas}

$$
P(l) = \left\{
\begin{array}{rcl}
0,& \mbox{se} & l\ <\ n\\
1,& \mbox{se} & l\ =\ n
\end{array}
\right.
$$

{\bf fator de redundância}
$$
B\ =\ |dado\ +\ red|\ /\ |dado|\ =\ n
$$

{\bf grau de reparação}
$$
d\ =\ 1
$$

{\bf disponibilidade de um dado}
$$
1 - q^n
$$

{\bf corrupção de um dado}
$$
q^n
$$

\begin{table}
\singlespacing
  \begin{center}
    \begin{tabular}{|p{5cm}||p{5cm}|}
      \hline  \hline
acesso & tupla $(r, w, a)$\\ \hline
criar & $(0, 0, n)$\\ \hline
apenas leitura & $(1, 0, 0)$\\ \hline
atualização consistente & $(1, n, 0)$\\ \hline
apagar & $(0, n, 0)$\\ \hline
    \end{tabular}
\caption{Operações sobre dados redundantes na replicação}
\label{tab2:comp}
  \end{center}
\end{table}

Comparando as tuplas (1, 0, 0) e (1, n, 0) da tabela ~\ref{tab2:comp}, podemos observar que a disponibilidade da operação "atualização consistente" é menor que a disponibilidade da operação "apenas leitura", quando o número de réplicas $n > 1$ é aplicado.

\subsubsection{Codificação por Apagamento}

Na Codificação (m, k), $k$ é o número de pedaços originais  e $m\ -\ k$ é o número de pedaços adicionados.

{\bf número de falhas suportadas} $l\ =\ m\ -\ k$

{\bf probabilidade de perda de dados, dado que ocorreram $l$ falhas}
$$
P(l) = \left\{
\begin{array}{rcl}
0,& \mbox{se} & l\ <=\ m\ -\ k\\
1,& \mbox{se} & l\ > m\ -\ k
\end{array}
\right.
$$

{\bf fator de redundância}
$$
B\ =\ m / k
$$

{\bf grau de reparação}
$$
d\ =\ k
$$

{\bf disponibilidade de um dado}
$$
B(k, m, p)\ =\ \sum_{i=k}^{m}{m \choose i}p^{i}q^{m-i}
$$

{\bf corrupção de um dado}
$$
q^{m-k}
$$

\begin{table}
\singlespacing
  \begin{center}
    \begin{tabular}{|p{5cm}||p{5cm}|}
      \hline  \hline
acesso & tupla $(r, w, a)$\\ \hline
criar & $(0, 0, n)$\\ \hline
apenas leitura & $(k, 0, 0)$\\ \hline
atualização consistente & $(k, m-k+1, k-1)$\\ \hline
apagar & $(0, m-k+1, 0)$\\ \hline
    \end{tabular}
\caption{Operações sobre dados redundantes na codificação por apagamento}
\label{tab3:comp}
  \end{center}
\end{table}

Por outro lado, comparando-se as tuplas $(k, 0, 0)$ e $(k, m-k+1, k-1)$ da tabela ~\ref{tab3:comp}, podemos observar que existe um caso no qual as operações "apenas leitura" e "atualização consistente" podem ter a mesma disponibilidade. Isto ocorre quando $m = 2k-1$ (assumindo-se também que número total de \emph{peers} disponíveis é maior ou igual a $m$), obtendo-se a tupla $(k, k, k-1)$.

\subsection{Sobrecarga de armazenamento}

Em \cite{Weatherspoon:2002:01, Dabek:2004}, os autores afirmam que a codificação consegue o mesmo nível de disponibilidade como a replicação, usando muito menos espaço de armazenamento.

\subsection{Disponibilidade dos peers}

\subsection{Leitura ou Atualização dos dados redundantes}

Em \cite{Chiola:2005}, os autores ainda concluem que:

\begin{itemize}
  \item o acesso somente de leitura pode ser suportado tanto por replicação de dados simples como por codificação
  \item para privilegiar atualização consistente, uma codificação de alta disponibilidade é necessária que se caracteriza por fracionamento do original dados em pedaços $k$ e adicionando exatamente $k-1$ pedaços
  \item se ler e a disponibilidade de atualização consistente são de igual importância, isso requer codificação $(2k-1, k)$
\end{itemize}

Podemos também concluir que a replicação é um caso onde $k = 1$ e $l\ =\ m - k\ = \ n\ -\ 1$, portanto $m\ =\ n$. 

Os autores também concluíram que usar apenas a replicação tem sentido apenas em poucos casos.

